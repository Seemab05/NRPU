{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaXU_4hS01bK"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boGNqouJ8brf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import random\n",
        "from scipy.spatial import distance\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import string\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import networkx as nx\n",
        "from keybert import KeyBERT\n",
        "from transformers import BertModel, BertTokenizer, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from flair.models import SequenceTagger\n",
        "from flair.data import Sentence\n",
        "from google.colab import drive\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Download spaCy model\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOY34KZJSlNh"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_lMxPnASi1D"
      },
      "source": [
        "# Importing all Datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1=pd.read_csv(r\"D:\\Sana\\Datasets\\dawn (full-data).csv\", encoding= 'utf-8')"
      ],
      "metadata": {
        "id": "CPssIGFt82NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2=pd.read_csv(r\"D:\\Sana\\Datasets\\pakistan_today(full-data).csv\", encoding= 'utf-8')"
      ],
      "metadata": {
        "id": "QNLfuvRj85D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3=pd.read_csv(r\"D:\\Sana\\Datasets\\tribune(full-data).csv\", encoding= 'utf-8')"
      ],
      "metadata": {
        "id": "cutHXF3y872M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4=pd.read_csv(r\"D:\\Sana\\Datasets\\daily_times(full-data).csv\", encoding= 'utf-8')"
      ],
      "metadata": {
        "id": "7fKiJIk_9BGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5=pd.read_csv(r\"D:\\Sana\\Datasets\\business_recorder(full-data).csv\", encoding= 'utf-8')"
      ],
      "metadata": {
        "id": "N9Xg-Aa_9FW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_unnamed(df):\n",
        "    return df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "# Remove 'Unnamed' columns from each DataFrame\n",
        "df1 = remove_unnamed(df1)\n",
        "df2 = remove_unnamed(df2)\n",
        "df3 = remove_unnamed(df3)\n",
        "df4 = remove_unnamed(df4)\n",
        "df5 = remove_unnamed(df5)\n",
        "\n",
        "# Display the shape of each DataFrame after removing 'Unnamed' columns\n",
        "print(\"Shape of df1:\", df1.shape)\n",
        "print(\"Shape of df2:\", df2.shape)\n",
        "print(\"Shape of df3:\", df3.shape)\n",
        "print(\"Shape of df4:\", df4.shape)\n",
        "print(\"Shape of df5:\", df5.shape)\n"
      ],
      "metadata": {
        "id": "AJLaUYi99Mms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Merge all three datasets"
      ],
      "metadata": {
        "id": "mHq2eU5o_ky8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thU1jWdMUtuv"
      },
      "outputs": [],
      "source": [
        "# Define the common date range for all DataFrames\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2023-05-31'\n",
        "\n",
        "# Filter and merge DataFrames\n",
        "merged_df = pd.concat([df[(df['date'] >= start_date) & (df['date'] <= end_date)] for df in [df1, df2, df3, df4, df5]])\n",
        "\n",
        "# Reset index\n",
        "merged_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Print the first few rows of the merged DataFrame\n",
        "print(merged_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df.loc[:, ~merged_df.columns.str.contains('^Unnamed')]"
      ],
      "metadata": {
        "id": "5P45rFKK9deD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Check for missing dates in each dataset"
      ],
      "metadata": {
        "id": "Q1LzycxDQeC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by 'source' and aggregate unique values in 'date'\n",
        "unique_dates_by_source = merged_df.groupby('source')['date'].unique()\n",
        "\n",
        "# Print the unique dates for each source\n",
        "for source, unique_dates in unique_dates_by_source.items():\n",
        "    print(f\"Source: {source}\")\n",
        "    print(f\"Unique Dates: {unique_dates}\")\n",
        "\n",
        "    # Generate a range of dates from January 2022 to December 2022\n",
        "    all_dates = pd.date_range(start='2022-01-01', end='2022-12-31')\n",
        "\n",
        "    # Find missing dates by comparing the range of dates with unique dates\n",
        "    missing_dates = all_dates.difference(unique_dates)\n",
        "\n",
        "    print(f\"Missing Dates: {missing_dates}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "JB9XUkCFTtxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Delete null rows and drop duplicates"
      ],
      "metadata": {
        "id": "mk9JViQmQ205"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape Before dropna: {merged_df.shape}\")\n",
        "merged_df = merged_df.dropna(how='all')\n",
        "merged_df = merged_df.dropna()\n",
        "print(f\"Shape After dropna: {merged_df.shape}\")"
      ],
      "metadata": {
        "id": "BO_O7id0T5Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape Before Duplicate Removal: {merged_df.shape}\")\n",
        "\n",
        "# Create a boolean mask to identify duplicated rows\n",
        "duplicates_mask = merged_df.duplicated(subset=['headline'], keep='first')\n",
        "\n",
        "# Filter and display the duplicated rows\n",
        "duplicates = merged_df[duplicates_mask]\n",
        "print(\"Duplicated Rows:\")\n",
        "print(duplicates)\n",
        "\n",
        "# Drop the duplicates\n",
        "merged_df.drop_duplicates(subset=['headline'], inplace=True)\n",
        "\n",
        "print(\".......................................\")\n",
        "print(f\"Shape After Duplicate Removal: {merged_df.shape}\")"
      ],
      "metadata": {
        "id": "pAbF0l_9Uplm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape Before Duplicate Removal: {merged_df.shape}\")\n",
        "merged_df.drop_duplicates(subset=['description'], inplace=True)\n",
        "#merged.drop_duplicates(inplace=True)\n",
        "print(\".......................................\")\n",
        "print(f\"Shape After Duplicate Removal: {merged_df.shape}\")"
      ],
      "metadata": {
        "id": "bPE2kmBu9eUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Handling Non-Ascii Characters"
      ],
      "metadata": {
        "id": "CVGNF-1K-Qya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_characters(text):\n",
        "    if isinstance(text, str):\n",
        "        #text = text.replace(\"Ã¢ÂÂ\", \"“\")\n",
        "        text = text.replace(\"Ã¢ÂÂ\", \"”\")\n",
        "        text = text.replace(\"Ã¢ÂÂ\", \"—\")\n",
        "        text = text.replace(\" Ã¢ÂÂ\", \"—\")\n",
        "        text = text.replace(\"Ã¢ÂÂ \", \"”\")\n",
        "        text = text.replace(\"Ã¢ÂÂ\", \"’\")  # Replace \"Ã¢ÂÂ\" with \"‘\"\n",
        "        text = text.replace(\"Ã¢ÂÂ\", \"‘\")\n",
        "        text = text.replace(\" Ã¢ÂÂ\", \"“\")\n",
        "        text = text.replace(\" Ã¢ÂÂ\", \"”\")\n",
        "        return text\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# Apply the replacement function to all columns in the DataFrame\n",
        "merged_df = merged_df.applymap(replace_characters)"
      ],
      "metadata": {
        "id": "lQEVxwN3-Ts9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_more_characters(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.replace(\"Ã¢ÂÂ\", \"“\")\n",
        "        text = text.replace(\"Ã¢Â£\", \"£\")\n",
        "        text = text.replace(\"Ã¢ÂÂ\", \"–\")\n",
        "        return text\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# Apply the replacement function to all columns in the DataFrame\n",
        "merged_df = merged_df.applymap(replace_more_characters)"
      ],
      "metadata": {
        "id": "4d0gDFtF-WEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def clean_non_Ascii(text):\n",
        "    if isinstance(text, str):\n",
        "        # Remove non-ASCII characters using a regular expression\n",
        "        cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "        # Remove extra whitespaces\n",
        "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
        "        return cleaned_text\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# Clean the 'News' column using the clean_non_Ascii function and print indices of changed rows\n",
        "changed_indices = []\n",
        "\n",
        "for idx, row in merged_df.iterrows():\n",
        "    original_text = row['description']\n",
        "    cleaned_text = clean_non_Ascii(original_text)\n",
        "\n",
        "    if original_text != cleaned_text:\n",
        "        merged_df.at[idx, 'description'] = cleaned_text\n",
        "        changed_indices.append(idx)\n",
        "\n",
        "print(f\"Changes have been made in rows with indices: {changed_indices}\")"
      ],
      "metadata": {
        "id": "VpUQZfFr-ZoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_categories = merged_df['categories'].unique()\n",
        "print(main_categories)"
      ],
      "metadata": {
        "id": "ZKqlMkw7-cOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLWb48SiSxzc"
      },
      "source": [
        "# Standardizing Categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPV2svLjSzx-"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "patterns_to_remove = [\n",
        "    r'00012\\. Where the taxable income exceeds Rs 21',\n",
        "    r'should be in accordance with the law and the constitution',\n",
        "    r'no foreign power shall be allowed to topple an elected government through a conspiracy',\n",
        "    r'and also — courtesy the word ‘shall’ — that he is bound to do so. Once the assembly opens',\n",
        "    r'within fourteen days of the receipt of the requisition.\"\"This means two things: that the Speaker has until March 22 to summon the assembly',\n",
        "    r'the constitution say that `` the speaker shall summon the national assembly to meet',\n",
        "    r'the opposition ha also moved to summon it . it’s submitted a requisition ( a fancy word for an official request ) under article 54 ( 3 ) . once a requisition is received',\n",
        "    r'that show president biden is willing to make difficult choices',\n",
        "    r'having said that',\n",
        "    r'which was presented on March 8',\n",
        "    r'Ã\\x99Â¾Ã\\x98Â§Ã\\x9AÂ©Ã\\x98Â³Ã\\x98ÂªÃ\\x98Â§Ã\\x99Â\\x86',\n",
        "    r'and also — courtesy the word ‘shall’ — that he is bound to do so. Once the assembly opens',\n",
        "    r'within fourteen days of the receipt of the requisition.\"\"This means two things: that the Speaker has until March 22 to summon the assembly',\n",
        "    r'the opposition ha also moved to summon it . it’s submitted a requisition ( a fancy word for an official request ) under article 54 ( 3 ) . once a requisition is received'\n",
        "    r'the opposition ha also moved to summon it . it’s submitted a requisition ( a fancy word for an official request ) under article 54 ( 3 ) . once a requisition is received',\n",
        "    r'the opposition ha also moved to summon it . it’s submitted a requisition \\( a fancy word for an official request \\) under article 54 \\( 3 \\) . once a requisition is received'\n",
        "    r'and also Ã¢Â\\x80Â\\x94 courtesy the word Ã¢Â\\x80Â\\x98shallÃ¢Â\\x80Â\\x99 Ã¢Â\\x80Â\\x94 that he is bound to do so. Once the assembly opens'\n",
        "    r' the opposition ha also moved to summon it . itÃ¢Â\\x80Â\\x99s submitted a requisition ( a fancy word for an official request ) under article 54 ( 3 ) . once a requisition is received '\n",
        "    r' the opposition ha also moved to summon it . it’s submitted a requisition \\( a fancy word for an official request \\) under article 54 \\( 3 \\) . once a requisition is received'\n",
        "    r' the opposition ha also moved to summon it . it’s submitted a requisition ( a fancy word for an official request ) under article 54 ( 3 ) . once a requisition is received '\n",
        "]\n",
        "# Create a combined regular expression pattern\n",
        "pattern = '|'.join(patterns_to_remove)\n",
        "\n",
        "# Use the pattern to filter the DataFrame\n",
        "merged_df = merged_df[~merged_df['categories'].str.contains(pattern, case=False, regex=True)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26rsbkAaS4hq"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the text to remove (with extra spaces)\n",
        "text_to_remove = '00012.   Where the taxable income exceeds        Rs 21'\n",
        "\n",
        "# Use the 'Category' column to filter the DataFrame\n",
        "merged_df = merged_df[~merged_df['categories'].str.replace(r'\\s', '', regex=True).str.contains(text_to_remove.replace(' ', ''))]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Pattern to remove\n",
        "pattern_to_remove = re.escape(\n",
        "    \"the opposition ha also moved to summon it . it’s submitted a requisition ( a fancy word for an official request ) under article 54 ( 3 ) . once a requisition is received\"\n",
        ")\n",
        "\n",
        "# Apply the pattern to remove the specified string\n",
        "merged_df['categories'] = merged_df['categories'].str.replace(pattern_to_remove, '', case=False, regex=True)\n"
      ],
      "metadata": {
        "id": "XDrEjigh-9Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(main_categories, columns=['categories'])"
      ],
      "metadata": {
        "id": "R7nHPSR_-_40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = r\"D:\\Sana\\Extra Material\\Ful_Data_Unique_Cat.csv\"\n",
        "unique_category_source_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f'DataFrame saved to: {output_path}')"
      ],
      "metadata": {
        "id": "nVDkYOKN_Dgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXBzb0xYS6vQ"
      },
      "outputs": [],
      "source": [
        "category_mappings = {\n",
        "    'Pakistan Today': {\n",
        "    'Sports': 'Others',\n",
        "    'NATIONAL & Top Headlines': 'National',\n",
        "    'NATIONAL & Top Headlines & World': 'National',\n",
        "    'Sports': 'Others',\n",
        "    'World': 'International',\n",
        "    'NATIONAL': 'National',\n",
        "    'NATIONAL & Top Headlines': 'National',\n",
        "    'Editorials & Opinion': 'Editorial',\n",
        "    'Comment & Opinion': 'Others',\n",
        "    'Opinion': 'Others',\n",
        "    'Top Headlines & World': 'International',\n",
        "    'Letters & Opinion': 'Others',\n",
        "    'ISLAMABAD & NATIONAL': 'National',\n",
        "    'Business': 'Business',\n",
        "    'Sports & Top Headlines': 'Others',\n",
        "    'NATIONAL & World': 'International',\n",
        "    'HEADLINES': 'Others',\n",
        "    'NATIONAL & Top Headlines & World': 'International',\n",
        "    'LAHORE & NATIONAL': 'National',\n",
        "    'HEADLINES & NATIONAL': 'National',\n",
        "    'CITY': 'National',\n",
        "    'NATIONAL & top Featured & World': 'International',\n",
        "    'KARACHI & NATIONAL': 'National',\n",
        "    'CITY & LAHORE': 'National',\n",
        "    'MULTAN & NATIONAL': 'National',\n",
        "    'HEADLINES & World': 'International',\n",
        "    'NATIONAL & Sports': 'Others',\n",
        "    'Uncategorized': 'Others',\n",
        "    'NATIONAL & top Featured': 'National',\n",
        "    'NATIONAL & top Featured & Top Headlines': 'National',\n",
        "    'HEADLINES & Sports': 'Others',\n",
        "    'CITY & HEADLINES': 'National',\n",
        "    'LAHORE & NATIONAL & Top Headlines': 'National',\n",
        "    'NATIONAL & PESHAWAR & Top Headlines': 'National',\n",
        "    'HEADLINES & NATIONAL & World': 'International',\n",
        "    'E-papers & Pakistan Today': 'Others',\n",
        "    'CITY & PESHAWAR': 'National',\n",
        "    'HEADLINES & NATIONAL & Top Headlines': 'National',\n",
        "    'LAHORE': 'National',\n",
        "    'NATIONAL & PESHAWAR': 'National',\n",
        "    'Sponsored Content': 'Others',\n",
        "    'Top Headlines': 'National',\n",
        "    'HEADLINES & Letters & Opinion': 'Others',\n",
        "    'CITY & LAHORE & Opinion': 'National',\n",
        "    'CITY & KARACHI & NATIONAL': 'National',\n",
        "    'HEADLINES & Top Headlines': 'National',\n",
        "    'ISLAMABAD & Pakistan Today': 'National',\n",
        "    'ISLAMABAD': 'National',\n",
        "    'MULTAN': 'National',\n",
        "    'PESHAWAR': 'National',\n",
        "    'top Featured & Top Headlines': 'National',\n",
        "    'top Featured': 'National',\n",
        "    'KARACHI': 'National',\n",
        "    'PESHAWAR & Sports': 'National',\n",
        "    'NATIONAL & Opinion': 'National',\n",
        "    'Sports & top Featured': 'Others',\n",
        "    'Comment & Letters': 'Others',\n",
        "    'Letters': 'Others',\n",
        "    'CITY & ISLAMABAD': 'National',\n",
        "    'CITY & ISLAMABAD & NATIONAL': 'National',\n",
        "    'CITY & KARACHI': 'National',\n",
        "    'Comment & NATIONAL & Opinion': 'National',\n",
        "    'NATIVE CONTENT': 'Others',\n",
        "    'NATIONAL & Top Non Business': 'National',\n",
        "    'Book Review & NATIONAL': 'Others',\n",
        "    'Entertainment & NATIONAL': 'Others',\n",
        "    'Comment': 'Others',\n",
        "    'Analysis & NATIONAL': 'Others',\n",
        "    'Editorials & Letters & Opinion': 'Editorial',\n",
        "    'Comment & HEADLINES & Opinion': 'Others',\n",
        "    'FEATURED & top Featured' : 'National',\n",
        "    'FEATURED & Top Non Business': 'National',\n",
        "    'FEATURED': 'National',\n",
        "    'GOVERNANCE': 'National',\n",
        "    'GOVERNANCE & HEADLINES': 'National',\n",
        "    'Top Non Business & World': 'International',\n",
        "    'GOVERNANCE & NATIONAL': 'National',\n",
        "    'HEADLINES & KARACHI': 'National',\n",
        "    'HEADLINES & LAHORE': 'National',\n",
        "    'HEADLINES & PESHAWAR': 'National',\n",
        "    'INTERVIEW & Top Headlines': 'Others',\n",
        "    'CITY & HEADLINES & LAHORE': 'National',\n",
        "    'KARACHI & LAHORE & NATIONAL': 'National',\n",
        "    'KARACHI & LAHORE': 'National',\n",
        "    'HEADLINES & NATIONAL & Top Non Business': 'National',\n",
        "    'FEATURED & Top Headlines': 'National',\n",
        "    'KARACHI & NATIONAL & Top Headlines': 'National',\n",
        "    'HEADLINES & Top Headlines & World': 'International',\n",
        "    'Editorials & HEADLINES & Opinion': 'National',\n",
        "    'HEADLINES & LAHORE & NATIONAL': 'National',\n",
        "    'Analysis & HEADLINES & NATIONAL & Top Headlines': 'National',\n",
        "    'Book Review': 'Others',\n",
        "    'HEADLINES & LAHORE & NATIONAL & Top Headlines': 'National',\n",
        "    'GOVERNANCE & HEADLINES & NATIONAL': 'National',\n",
        "    'LAHORE & NATIONAL & PESHAWAR': 'National',\n",
        "    'Entertainment': 'Others',\n",
        "    'Comment & Editorials': 'Others',\n",
        "    'HEADLINES & Sports & Top Headlines': 'Others',\n",
        "    'SPONSORED': 'Others',\n",
        "    'CITY & HEADLINES & NATIONAL': 'National',\n",
        "    'CITY & NATIONAL': 'National',\n",
        "    'FEATURED & NATIONAL': 'National',\n",
        "    'ISLAMABAD & KARACHI & LAHORE': 'National',\n",
        "    'HEADLINES & NATIONAL & Top Headlines & World': 'National',\n",
        "    'LAHORE & Top Headlines': 'National',\n",
        "    'HEADLINES & ISLAMABAD & NATIONAL': 'National',\n",
        "    'HEADLINES & NATIONAL & Sports & Top Headlines': 'Others',\n",
        "    'NATIONAL & Sports & Top Headlines': 'Others',\n",
        "    'Editorials':'Editorial',\n",
        "    'Sports & World': 'Others',\n",
        "    'HEADLINES & ISLAMABAD & LAHORE & NATIONAL': 'National',\n",
        "    'CITY & FEATURED & LAHORE': 'National',\n",
        "    'Entertainment & World': 'Others',\n",
        "    'HEADLINES & ISLAMABAD & NATIONAL & Pakistan Today': 'National',\n",
        "    'NATIONAL & Sports & Top Headlines & Top Non Business': 'Others',\n",
        "    'HEADLINES & KARACHI & NATIONAL': 'National',\n",
        "    'OIC & World': 'International',\n",
        "    'Agriculture & NATIONAL': 'National',\n",
        "    'CITY & Education & MULTAN': 'National',\n",
        "    'Cartoon & Letters': 'Others',\n",
        "    'FEATURED & HEADLINES & NATIONAL': 'National',\n",
        "    'FEATURED & Sports': 'Others',\n",
        "    'ISLAMABAD & SPONSORED': 'National',\n",
        "    'top Featured & World': 'International',\n",
        "    'HEADLINES & NATIONAL & top Featured': 'National',\n",
        "    'NATIONAL & Pakistan Today': 'National',\n",
        "    'Cartoon & Editorials': 'Others',\n",
        "    'CITY & LAHORE & NATIONAL': 'National',\n",
        "    'Education & NATIONAL': 'National',\n",
        "\n",
        "\n",
        "},\n",
        "        'Business Recorder': {\n",
        "        'Markets ': 'Business',\n",
        "        'Markets': 'Business',\n",
        "        'MARKETS': 'Business',\n",
        "        'MARKETS ': 'Business',\n",
        "        'Business': 'Business',\n",
        "        'Business ': 'Business',\n",
        "        'World ': 'International',\n",
        "        'World': 'International',\n",
        "        'Pakistan': 'National',\n",
        "        'Pakistan ': 'National',\n",
        "        'Sports ': 'Others',\n",
        "        'Sports': 'Others',\n",
        "        'Editorials': 'Editorial',\n",
        "        'Editorials ': 'Editorial',\n",
        "        'Business & Finance ': 'Business',\n",
        "        'Business & Finance': 'Business',\n",
        "        'Perspectives': 'Others',\n",
        "        'Perspectives ': 'Others',\n",
        "        'BR Research': 'Others',\n",
        "        'BR Research ': 'Others',\n",
        "        'Life & Style': 'Others',\n",
        "        'Life & Style ': 'Others',\n",
        "        'Opinion': 'Opinion',\n",
        "        'Opinion ': 'Opinion',\n",
        "        'Technology': 'Others',\n",
        "        'Technology ': 'Others',\n",
        "        'Print': 'Print',\n",
        "        'Print ': 'Print',\n",
        "        'Top Stories': 'National',\n",
        "        'Top News': 'National',\n",
        "        'Budget': 'National',\n",
        "        'Epaper': 'Others',\n",
        "        'Brief Recordings': 'Others',\n",
        "        'Entertainment': 'Others',\n",
        "        'Supplements': 'Business',\n",
        "        'Weather': 'Others',\n",
        "        'US Elections 2020': 'Others',\n",
        "        'Asia Cup': 'Others',\n",
        "        'Budget 2022-23': 'Business',\n",
        "        'Budget 2021-22': 'Business',\n",
        "        'Top Stories ': 'National',\n",
        "        'Top News ': 'National',\n",
        "        'Budget ': 'National',\n",
        "        'Epaper ': 'Others',\n",
        "        'Brief Recordings ': 'Others',\n",
        "        'Entertainment ': 'Others',\n",
        "        'Supplements ': 'Business',\n",
        "        'Weather ': 'Others',\n",
        "        'US Elections 2020 ': 'Others',\n",
        "        'Asia Cup ': 'Others',\n",
        "        'Budget 2022-23 ': 'Business',\n",
        "        'Budget 2021-22 ': 'Business'\n",
        "    },\n",
        "    'Dawn': {\n",
        "        'Pakistan': 'National',\n",
        "        'Pakistan ': 'National',\n",
        "        'Business': 'Business',\n",
        "        'Business ': 'Business',\n",
        "        'World': 'International',\n",
        "        'World ': 'International',\n",
        "        'Prism': 'Others',\n",
        "        'Prism ': 'Others',\n",
        "        'Sport': 'Others',\n",
        "        'Sport ': 'Others'\n",
        "    },\n",
        "    'Daily Times': {\n",
        "'Pakistan':'National',\n",
        "'Reviews'\n",
        "'Arts, Culture &amp; Books':'Others',\n",
        "'Infotainment':'Others',\n",
        "'Perspectives':'Others',\n",
        "'Commentary / Insight':'Others',\n",
        "'Letters':'Others',\n",
        "'Op-Ed':'Others',\n",
        "'Editorial':'Editorial',\n",
        "'Pakistan & Top Stories':'National',\n",
        "'Business':'Business',\n",
        "'Blogs':'Others',\n",
        "'World':'International',\n",
        "'Pakistan & World':'National',\n",
        "'Pakistan & Uncategorized':'National',\n",
        "'Top Stories':'Others',\n",
        "'Trending & World':'International',\n",
        "'Top Stories & World':'International',\n",
        "'Lifestyle':'Others',\n",
        "'Sports':'Others',\n",
        "'Trending':'Others',\n",
        "'Infotainment & Trending':'Others',\n",
        "'Pakistan & Trending':'National',\n",
        "'Uncategorized':'Others',\n",
        "'Islamabad':'National',\n",
        "'Sindh':'National',\n",
        "'Punjab':'National',\n",
        "'Punjab & Trending':'National',\n",
        "'Blogs & Trending':'Others',\n",
        "'Pakistan & Top Stories & World':'National',\n",
        "'Uncategorized & World':'International',\n",
        "'Islamabad & Pakistan':'National',\n",
        "'Khyber Pakhtunkhwa & Pakistan':'National',\n",
        "'Pakistan & Punjab':'National',\n",
        "'Business & Pakistan':'Business',\n",
        "'Balochistan & Pakistan':'National',\n",
        "'Business & World':'Business',\n",
        "'Sports & Top Stories':'Others',\n",
        "'Business & Trending':'Business',\n",
        "'Business & Top Stories':'Business',\n",
        "'Khyber Pakhtunkhwa':'National',\n",
        "'Pakistan & Sports':'Others',\n",
        "'Sports & Uncategorized':'Others',\n",
        "'Featured':'Others',\n",
        "'Balochistan & Reviews':'National',\n",
        "'Sponsored Content':'Others',\n",
        "'Sports & Trending':'Others',\n",
        "'Business & Pakistan & Top Stories':'Business',\n",
        "'Infotainment & Top Stories':'Others',\n",
        "'Perspectives & World':'International',\n",
        "'Entertainment':'Others',\n",
        "'Pakistan & Sponsored Content':'National',\n",
        "'Business & Uncategorized':'Business',\n",
        "'Featured & World':'International',\n",
        "'Lifestyle & Trending':'Others',\n",
        "'Trending & Trending & World':'International',\n",
        "'Pakistan & Sindh':'National',\n",
        "'Infotainment & Pakistan':'Others',\n",
        "'Pakistan & Trending & Uncategorized':'National',\n",
        "'Business & Sponsored Content':'Business',\n",
        "'Arts, Culture &amp; Books & Top Stories':'Others',\n",
        "'Pakistan & Sports & World':'Others',\n",
        "'International':'International',\n",
        "'Lifestyle & Uncategorized':'Others',\n",
        "'Top Stories & Uncategorized & World':'International',\n",
        "'Lifestyle & Top Stories':'Others',\n",
        "'Lifestyle & Pakistan':'Others',\n",
        "'Op-Ed & Pakistan':'National',\n",
        "'Cartoons':'Others',\n",
        "'World & World':'International',\n",
        "'Pakistan & Pakistan & Uncategorized':'National',\n",
        "'Entertainment & World':'Others',\n",
        "'Health & World':'International',\n",
        "'Health':'Others',\n",
        "'Blog & Blogs':'Others',\n",
        "'Pakistan & Uncategorized & World':'National',\n",
        "'Pakistan & Top Stories & Uncategorized':'National',\n",
        "'Pakistan & Reviews':'National',\n",
        "'Film And Drama Reviews':'Others',\n",
        "'Lahore':'National',\n",
        "'Entertainment & Lifestyle & Trending':'Others',\n",
        "'Lahore & Pakistan':'National',\n",
        "'Sports & Top Stories & Trending':'Others',\n",
        "'Pakistan & Sindh & Top Stories':'National',\n",
        "'Top Stories & Trending & World':'International',\n",
        "'Entertainment & Pakistan & Trending':'Others',\n",
        "'Pakistan & Top Stories & Trending':'National',\n",
        "'Pakistan & Sindh & Top Stories & Trending':'National',\n",
        "'Sports & World':'Others',\n",
        "'Arts, Culture &amp; Books & Trending':'Others',\n",
        "'Entertainment & Lifestyle':'Others',\n",
        "'Pakistan & Sports & Top Stories':'Others',\n",
        "'Sponsored Content & Trending':'Others',\n",
        "'Entertainment & Trending':'Others',\n",
        "'Entertainment & Trending & World':'Others',\n",
        "'Sports & Top Stories & World':'Others',\n",
        "'Entertainment & Pakistan & Top Stories':'Others',\n",
        "'Entertainment & Top Stories':'Others',\n",
        "'Blog':'Others',\n",
        "'Culture':'Others',\n",
        "'Pakistan & Punjab & Top Stories':'National',\n",
        "'Lahore & Pakistan & Punjab & Top Stories':'National',\n",
        "'Islamabad & Pakistan & Top Stories':'National',\n",
        "'International & World':'International',\n",
        "'International & Top Stories & World':'International',\n",
        "'Pakistan & Pakistan & Top Stories':'National',\n",
        "'Pakistan & Pakistan':'National',\n",
        "'Pakistan & Pakistan & Sindh & Top Stories':'National',\n",
        "'International & Top Stories':'International',\n",
        "'International & World & World':'International',\n",
        "'Business & Perspectives':'Business',\n",
        "'Entertainment & Music & Uncategorized':'Others',\n",
        "'Sports & Sports':'Others',\n",
        "'Featured & Lahore':'National',\n",
        "'Top Stories & World & World':'International',\n",
        "'Entertainment & TV':'Others',\n",
        "'TV':'Others',\n",
        "'Sports & Sports & Uncategorized':'Others',\n",
        "'Entertainment & International':'Others',\n",
        "'Business & Lahore & Pakistan & Punjab':'Business',\n",
        "'Business & Punjab':'Business',\n",
        "'Pakistan & Punjab & World':'National',\n",
        "'Lahore & Pakistan & Pakistan':'National',\n",
        "'Featured & Gilgit Baltistan & Pakistan & Pakistan & Trending':'National',\n",
        "'Entertainment & Music':'Others',\n",
        "'Entertainment & Fashion & Lifestyle':'Others',\n",
        "'Business & Islamabad & Pakistan':'Business',\n",
        "'Islamabad & Pakistan & Punjab':'National',\n",
        "'Pakistan & Pakistan & Punjab & Top Stories':'National',\n",
        "'Balochistan & Pakistan & Pakistan & Top Stories':'National',\n",
        "'Lifestyle & TV':'Others',\n",
        "'Islamabad & Pakistan & Pakistan & Top Stories':'National',\n",
        "'Pakistan & Pakistan & World':'National',\n",
        "'Entertainment & Film And Drama Reviews':'Others',\n",
        "'Business & Khyber Pakhtunkhwa':'Business',\n",
        "'Balochistan':'National',\n",
        "'Balochistan & Business':'Business',\n",
        "'Lahore & Punjab':'National',\n",
        "'Khyber Pakhtunkhwa & Pakistan & Pakistan':'National',\n",
        "'Business & Pakistan & Uncategorized':'Business',\n",
        "'Lahore & Pakistan & Top Stories':'National',\n",
        "'Entertainment & Lifestyle & TV':'Others',\n",
        "'Lifestyle & Music':'Others',\n",
        "'Khyber Pakhtunkhwa & Pakistan & Top Stories':'National',\n",
        "'Business & Pakistan & Punjab':'Business',\n",
        "'Business & Pakistan & Sindh & Top Stories':'Business',\n",
        "'Balochistan & Islamabad & Pakistan & Pakistan':'National',\n",
        "'Islamabad & Pakistan & Sindh':'National',\n",
        "'Lifestyle & Movies':'Others',\n",
        "'Balochistan & Islamabad & Punjab & Sindh':'National',\n",
        "'Balochistan & Islamabad & Khyber Pakhtunkhwa & Pakistan & Punjab & Sindh & Top Stories':'National',\n",
        "'Featured & Pakistan & Top Stories':'National',\n",
        "'Entertainment & Lifestyle & Music':'Others',\n",
        "'Entertainment & Lifestyle & Movies':'Others',\n",
        "'Business & Pakistan & Pakistan':'Business',\n",
        "'Health & Pakistan & Sindh':'National',\n",
        "'Islamabad & Pakistan & Pakistan':'National',\n",
        "'Khyber Pakhtunkhwa & Pakistan & Pakistan & Top Stories':'National',\n",
        "'Business & Islamabad & Pakistan & Pakistan':'Business',\n",
        "'Islamabad & Pakistan & Pakistan & Sindh & Top Stories':'National',\n",
        "'Islamabad & Pakistan & Pakistan & Punjab & Sindh & Top Stories':'National',\n",
        "'Pakistan & Pakistan & Punjab':'Business',\n",
        "'Business & Islamabad & Pakistan & Pakistan & Top Stories':'Business',\n",
        "'Business & Pakistan & Pakistan & Top Stories':'Business',\n",
        "'Commentary / Insight & Trending':'Others',\n",
        "'Pakistan & Punjab & Trending':'National',\n",
        "'Health & Pakistan & Punjab':'National',\n",
        "'Lahore & Pakistan & Punjab':'National',\n",
        "'Business & Pakistan & Punjab & Top Stories':'Business',\n",
        "'Balochistan & Islamabad & Khyber Pakhtunkhwa & Pakistan & Pakistan & Punjab & Sindh & Top Stories':'National',\n",
        "'Pakistan & Pakistan & Sindh':'National',\n",
        "'Balochistan & Pakistan & Pakistan':'National',\n",
        "'Islamabad & Pakistan & Pakistan & Punjab & Top Stories':'National',\n",
        "'Featured & Pakistan & Punjab':'National',\n",
        "'Business & Pakistan & Pakistan & Punjab':'Business',\n",
        "'Khyber Pakhtunkhwa & Pakistan & Pakistan & Sindh & Top Stories':'National',\n",
        "'Featured & Pakistan & Trending & Uncategorized':'National',\n",
        "'Top Stories & Trending & Trending & World':'International',\n",
        "'Fashion & Lifestyle':'Others',\n",
        "'Balochistan & Business & Pakistan & Pakistan & Top Stories':'Business',\n",
        "'Business & Featured & Pakistan':'Business',\n",
        "'Business & Pakistan & Sindh':'Business',\n",
        "'Health & Khyber Pakhtunkhwa & Pakistan':'National',\n",
        "'Gossip & Lifestyle':'Others',\n",
        "'Lifestyle & Trending & TV':'Others',\n",
        "'Lifestyle & Movies & Music & Trending':'Others',\n",
        "'Pakistan & Pakistan & Top Stories & World':'National',\n",
        "'Health & Pakistan':'National',\n",
        "'Gossip & Lifestyle & Trending':'Others',\n",
        "'Business & Pakistan & Pakistan & Punjab & Top Stories':'Business',\n",
        "'Health & Pakistan & Top Stories':'National',\n",
        "'Celebrity Interviews & Featured & Lifestyle':'Others',\n",
        "'Business & Sindh':'Business',\n",
        "'Lifestyle & Movies & Pakistan':'Others',\n",
        "'Gossip & International & Lifestyle & Trending':'Others',\n",
        "'Fashion & Featured & Gossip & Lifestyle & Trending':'Others',\n",
        "'Entertainment & Gossip & Lifestyle':'Others',\n",
        "'Pakistan & Pakistan & Sindh & Top Stories & Trending':'National',\n",
        "'Khyber Pakhtunkhwa & Pakistan & Pakistan & Punjab & Sindh & Top Stories':'National',\n",
        "'Business & Pakistan & Pakistan & Sindh':'Business',\n",
        "'Pakistan & Sindh & Uncategorized':'National',\n",
        "'Lifestyle & Music & TV':'Others',\n",
        "'Balochistan & Khyber Pakhtunkhwa & Pakistan & Pakistan & Punjab & Sindh & Top Stories':'National',\n",
        "'Commentary / Insight & Pakistan':'Others',\n",
        "'Lifestyle & Movies & Trending':'Others',\n",
        "'Business & Top Stories & World':'Business',\n",
        "'Fashion & Lifestyle & Trending':'Others',\n",
        "'Business & Khyber Pakhtunkhwa & Pakistan':'Business',\n",
        "'Gilgit Baltistan & Pakistan':'National',\n",
        "'Islamabad & Pakistan & Punjab & Top Stories':'National',\n",
        "'Business & Pakistan & Pakistan & Sindh & Top Stories':'Business',\n",
        "'Balochistan & Business & Pakistan & Pakistan':'Business',\n",
        "'Featured & Pakistan':'National',\n",
        "'Pakistan & Pakistan & World & World':'National',\n",
        "'Business & Health & Pakistan':'Business',\n",
        "'Top Stories & Uncategorized':'National',\n",
        "'Khyber Pakhtunkhwa & Pakistan & Punjab':'National',\n",
        "'International & Pakistan & Top Stories':'National',\n",
        "'Pakistan & Punjab & Sindh & Top Stories':'National',\n",
        "'Punjab & Top Stories':'National',\n",
        "'Balochistan & Khyber Pakhtunkhwa & Pakistan & Punjab & Sindh & Top Stories':'National',\n",
        "'Punjab & Sindh':'National',\n",
        "'Balochistan & Pakistan & Top Stories':'National',\n",
        "'Pakistan & Pakistan & Sports':'Others',\n",
        "'Khyber Pakhtunkhwa & Pakistan & Sindh & Top Stories':'National',\n",
        "'Business & Sindh & Top Stories':'Business',\n",
        "'Business & Islamabad':'Business',\n",
        "'Khyber Pakhtunkhwa & Pakistan & Sindh':'National',\n",
        "'Pakistan & Perspectives':'National',\n",
        "'Pakistan & Sindh & Trending':'National',\n",
        "'Sindh & Sports':'Others',\n",
        "'Lahore & Top Stories':'National',\n",
        "'Sindh & Top Stories':'National',\n",
        "'Lifestyle & World':'Others',\n",
        "'Balochistan & Top Stories':'National',\n",
        "'Celebrity Interviews & Lifestyle':'Others',\n",
        "'Pakistan & Trending & Trending':'National',\n",
        "'Gilgit Baltistan & Pakistan & Top Stories':'National',\n",
        "'Gilgit Baltistan & Pakistan & Punjab':'National',\n",
        "'Arts, Culture &amp; Books & Lifestyle':'Others',\n",
        "'International & Pakistan':'National',\n",
        "'Health & Pakistan & World':'National',\n",
        "'Featured & Lifestyle':'Others',\n",
        "'Entertainment & Lifestyle & Pakistan':'Others',\n",
        "'Lifestyle & Lifestyle':'Others',\n",
        "'Gilgit Baltistan':'National',\n",
        "'Islamabad & Top Stories':'National',\n",
        "'Pakistan & Sports & Sports':'Others',\n",
        "'Pakistan & Sports & Uncategorized':'Others',\n",
        "'International & Pakistan & World':'International',\n",
        "'Business & Sports':'Business',\n",
        "'Karachi':'National',\n",
        "'Lifestyle & Sports':'Others',\n",
        "'Karachi & Pakistan & Top Stories':'National',\n",
        "'Business & Lifestyle & Pakistan':'Business',\n",
        "'Karachi & Pakistan & Sindh':'National',\n",
        "'Blogs & Lifestyle':'Others',\n",
        "'Karachi & Sindh':'National',\n",
        "'Islamabad & Lahore & Pakistan':'National',\n",
        "'Karachi & Pakistan':'National',\n",
        "'International & Sports':'Others',\n",
        "'Finance':'Business',\n",
        "'Featured & Pakistan & World':'National',\n",
        "'Pakistan & Trending & Trending & World':'National',\n",
        "'Business & International':'Business',\n",
        "'Entertainment & Pakistan':'Others',\n",
        "'Fashion':'Others',\n",
        "'Balochistan & Business & Pakistan & Top Stories':'Business',\n",
        "'Khyber Pakhtunkhwa & Top Stories':'National',\n",
        "'Culture & World':'Others',\n",
        "'Fashion & Gilgit Baltistan':'Others',\n",
        "'Science and Technology':'National',\n",
        "'Travel':'Others',\n",
        "'Health & Punjab':'National',\n",
        "'Off-beat & World':'Others',\n",
        "'Karachi & Sindh & Top Stories':'National',\n",
        "'Business & Finance & Pakistan':'Business',\n",
        "'Business & Finance':'Business',\n",
        "'International & Movies':'Others',\n",
        "'Kashmir & Pakistan':'National',\n",
        "'International & Lifestyle':'Others',\n",
        "'Pakistan & Punjab & Sindh':'National',\n",
        "'Arts, Culture &amp; Books & Entertainment & World':'Others',\n",
        "'Featured & International & Lifestyle':'Others',\n",
        "'Arts, Culture &amp; Books & Pakistan':'Others',\n",
        "'Entertainment & Infotainment & International & Science and Technology & Social Mania & World':'Others',\n",
        "'Entertainment & Karachi & Sindh':'Others',\n",
        "'International & Lifestyle & Pakistan':'Others',\n",
        "'Arts, Culture &amp; Books & Lifestyle & Pakistan':'Others',\n",
        "'Off-beat & Top Stories':'Others',\n",
        "'Entertainment & Lifestyle & World':'Others',\n",
        "'Infotainment & Lifestyle':'Others',\n",
        "'Business & Pakistan & Science and Technology':'Business',\n",
        "'International & Science and Technology':'International',\n",
        "'Education & Pakistan':'National',\n",
        "'Kashmir':'National',\n",
        "'Arts, Culture &amp; Books & Entertainment':'Others',\n",
        "'Pakistan & Travel':'National',\n",
        "'Travel & World':'Others',\n",
        "'Science and Technology & World':'International',\n",
        "'Fashion & International & Lifestyle':'Others',\n",
        "'Entertainment & Infotainment & International':'Others',\n",
        "'Blogs & Perspectives':'Others',\n",
        "'Infotainment & Lahore & Pakistan':'Others',\n",
        "'Health & International':'International',\n",
        "'Infotainment & International & World':'Others',\n",
        "'Pakistan & Science and Technology':'National',\n",
        "'Culture & Infotainment & Pakistan':'Others',\n",
        "'Balochistan & Health':'National',\n",
        "'Top Stories & Travel':'Others',\n",
        "'Business & Finance & International':'Business',\n",
        "'Infotainment & Lifestyle & World':'Others',\n",
        "'Arts, Culture &amp; Books & Featured & Featured':'Others',\n",
        "'Film And Drama Reviews & International & Lifestyle':'Others',\n",
        "'Education & Lahore & Pakistan':'National',\n",
        "'Education & Pakistan & Top Stories':'National',\n",
        "'Blog & Lifestyle & Pakistan':'Others',\n",
        "'Blogs & Culture':'Others',\n",
        "'Business & Finance & International & Pakistan':'Business',\n",
        "'Education':'Others',\n",
        "'Celebrity Interviews & Lifestyle & Pakistan':'Others',\n",
        "'Infotainment & World':'Others',\n",
        "'Education & International':'International',\n",
        "'Lifestyle & Pakistan & Trending':'Others',\n",
        "'Off-beat & Top Stories & World':'Others',\n",
        "'Health & Lahore & Pakistan & Punjab & Top Stories':'National',\n",
        "'Pakistan & Top Stories & Travel':'National',\n",
        "'Health & Lahore & Pakistan & Top Stories':'National',\n",
        "'Education & Pakistan & Punjab':'National',\n",
        "'Business & Pakistan & World':'Business',\n",
        "'Off-beat':'Others',\n",
        "'Health & Islamabad & Pakistan & Top Stories':'National',\n",
        "'International & Lifestyle & Movies':'Others',\n",
        "'Celebrity Interviews & International & Lifestyle':'Others',\n",
        "'Off-beat & Pakistan':'Others',\n",
        "'Business & Reviews':'Business',\n",
        "'Fashion & Lifestyle & Pakistan':'Others',\n",
        "'Business & Finance & Top Stories':'Business',\n",
        "'Health & Lahore & Top Stories':'National',\n",
        "'Health & Pakistan & Punjab & Top Stories':'National',\n",
        "'Health & Social Mania':'Others',\n",
        "'Reviews & Sports':'Others',\n",
        "'International & Kashmir':'International',\n",
        "'Business & Finance & Pakistan & Top Stories':'Business',\n",
        "'Business & International & Pakistan':'Business',\n",
        "'Islamabad & Lifestyle & Pakistan':'Others',\n",
        "'Arts, Culture &amp; Books & Health':'Others',\n",
        "'Lahore & Pakistan & Sports':'Others',\n",
        "'Islamabad & Pakistan & Sports':'Others',\n",
        "'Entertainment & Karachi & Pakistan & Sports & Top Stories':'National',\n",
        "'Lahore & Pakistan & Sports & Top Stories':'National',\n",
        "'Balochistan & Education':'National',\n",
        "'Islamabad & Lahore & Pakistan & Sports & Top Stories':'Others',\n",
        "'Islamabad & Lahore & Sports & Top Stories':'National',\n",
        "'International & Travel':'International',\n",
        "'Islamabad & Sports':'National',\n",
        "'Islamabad & Lahore & Pakistan & Sports':'Others',\n",
        "'Off-beat & Uncategorized':'National',\n",
        "'Karachi & Pakistan & Sindh & Sports':'Others',\n",
        "'Pakistan & World & World':'National',\n",
        "'Arts, Culture &amp; Books & Culture':'Others',\n",
        "'Health & Top Stories & World':'International',\n",
        "'Kashmir & Pakistan & Top Stories':'National',\n",
        "'Kashmir & Pakistan & World':'National',\n",
        "'Health & Top Stories':'National',\n",
        "'Islamabad & Pakistan & Trending':'National',\n",
        "'Gilgit Baltistan & Pakistan & Sports':'Others',\n",
        "'Kashmir & Top Stories & World':'National',\n",
        "'Health & Islamabad & Pakistan & Pakistan':'National',\n",
        "'Music':'Others',\n",
        "'Islamabad & Pakistan & World':'National',\n",
        "'International & Islamabad & Pakistan':'National',\n",
        "'Pakistan & Sindh & Sports':'Others',\n",
        "'Health & Islamabad':'National',\n",
        "'Arts, Culture &amp; Books & Perspectives':'Others',\n",
        "'Arts, Culture &amp; Books & Featured':'Others',\n",
        "'Kashmir & Top Stories':'National',\n",
        "'Karachi & Pakistan & Sindh & Top Stories':'National',\n",
        "'Lifestyle & Reviews':'Others',\n",
        "'Health & Islamabad':'National',\n",
        "'Islamabad & Lifestyle':'Others',\n",
        "'Business & Finance & Pakistan & Uncategorized':'Business',\n",
        "'Social Mania & TGIF & Trending & World':'Others',\n",
        "'Business & Science and Technology & TGIF & World':'Business',\n",
        "'Culture & Pakistan':'Others',\n",
        "'Lifestyle & Science and Technology & Sports':'Others',\n",
        "'Business & Trending & World':'Business',\n",
        "'Business & Top Stories & Trending & Uncategorized & World':'Business',\n",
        "'Business & Top Stories & Trending & World':'Business',\n",
        "'Business & Pakistan & Sports':'Business',\n",
        "'Business & Pakistan & Trending & World':'Business',\n",
        "'Business & Education & Pakistan & Trending':'Business',\n",
        "'Islamabad & Lahore & Pakistan & Top Stories':'National',\n",
        "'Health & Khyber Pakhtunkhwa':'National',\n",
        "'Islamabad & Kashmir & Top Stories':'National',\n",
        "'Lahore & Punjab & Top Stories':'National',\n",
        "'Punjab & Uncategorized':'National',\n",
        "'Education & Punjab':'National',\n",
        "'Education & Pakistan & Trending & World':'National',\n",
        "'Arts, Culture &amp; Books & Education':'Others',\n",
        "'Gilgit Baltistan & Top Stories':'National',\n",
        "'Blogs & Pakistan':'National',\n",
        "'Arts, Culture &amp; Books & Blog':'Others',\n",
        "'Education & Top Stories':'Others',\n",
        "'Featured & Op-Ed':'Others',\n",
        "'Blogs & Sports':'Others',\n",
        "'Blogs & Featured':'Others',\n",
        "'Science and Technology & Top Stories':'Others',\n",
        "'Lifestyle & Pakistan & Top Stories':'National',\n",
        "'Blogs & Lifestyle & Uncategorized':'Others',\n",
        "'Science and Technology & Trending':'Others',\n",
        "'Education & International & World':'International',\n",
        "'Infotainment & Science and Technology':'Others',\n",
        "'Balochistan & Pakistan & Sindh':'National',\n",
        "'Balochistan & Sports':'Others',\n",
        "'International & Karachi & Lifestyle':'Others',\n",
        "'Balochistan & Health & Pakistan':'National',\n",
        "'Lifestyle & Top Stories & Uncategorized':'Others',\n",
        "'Blogs & Gilgit Baltistan & Sports':'Others',\n",
        "'Entertainment & International & Lifestyle':'Others',\n",
        "'Gossip':'Others',\n",
        "'Kashmir & Pakistan & Uncategorized':'National',\n",
        "'Region':'Others',\n",
        "'International & Science and Technology & World':'International',\n",
        "'Infotainment & International':'Others',\n",
        "'Arts, Culture &amp; Books & World':'Others',\n",
        "'Entertainment & Science and Technology & Top Stories':'Others',\n",
        "'Featured & Uncategorized':'Others',\n",
        "'Entertainment & Infotainment':'Others',\n",
        "'Entertainment & Lifestyle & Lifestyle':'Others',\n",
        "'sci-tec & World':'Others',\n",
        "'Health & Lifestyle':'Others',\n",
        "'Entertainment & Lifestyle & Uncategorized':'Others',\n",
        "  },\n",
        "    'Tribune': {\n",
        "'Pakistan, Khyber Pakhtunkhwa, Dera Ismail Khan ':'National',\n",
        "'Pakistan, Khyber Pakhtunkhwa, Dera Ismail Khan':'National',\n",
        " 'Pakistan, Punjab':'National',\n",
        " 'World':'International',\n",
        " 'Pakistan, K-P':'National',\n",
        " 'Sports':'Others',\n",
        " 'Pakistan, Business':'Business',\n",
        " 'Business':'Business',\n",
        " 'Pakistan':'National',\n",
        " 'Life & Style, Film, Gossip':'Others',\n",
        " 'Food':'Others',\n",
        " 'Life & Style, TV':'Others',\n",
        " 'Technology':'Others',\n",
        " 'Sindh':'National',\n",
        " 'Life & Style, Film':'Others',\n",
        " 'Life & Style, Gossip':'Others',\n",
        " 'Life & Style, Music':'Others',\n",
        " 'Punjab':'National',\n",
        " 'K-P':'National',\n",
        " 'Opinion':'Others',\n",
        " 'Editorial':'Editorial',\n",
        " 'Balochistan':'National',\n",
        " 'Sindh, Health':'National',\n",
        " 'Pakistan, Life & Style':'Others',\n",
        " 'Jammu & Kashmir, Health':'National',\n",
        " 'Pakistan, Sindh':'National',\n",
        " 'K-P, Music':'Others',\n",
        " 'World, Jammu & Kashmir':'International',\n",
        " 'Life & Style':'Others',\n",
        " 'Pakistan, Balochistan':'National',\n",
        " 'Business, World':'National',\n",
        " 'Pakistan, World':'National',\n",
        " 'Gilgit Baltistan':'National',\n",
        " 'Jammu & Kashmir':'National',\n",
        " 'World, Technology':'International',\n",
        " 'Life & Style, Art and Books, Music':'Others',\n",
        " 'Life & Style, Fashion, Gossip':'Others',\n",
        " 'Life & Style, Music, Gossip':'Others',\n",
        " 'Pakistan, Jammu & Kashmir':'National',\n",
        " 'Sindh, Punjab':'National',\n",
        " 'Life & Style, Fashion':'Others',\n",
        " 'Life & Style, Film, TV':'Others',\n",
        " 'Balochistan, Business':'Business',\n",
        " 'Life & Style, Health':'Others',\n",
        " 'World, Sports':'Others',\n",
        " 'Punjab, Business':'Business',\n",
        " 'Music, Film':'Others',\n",
        " 'TV':'Others',\n",
        " 'Life & Style, Music, Food':'Others',\n",
        " 'Pakistan, Health':'National',\n",
        " 'World, Gilgit Baltistan':'National',\n",
        " 'World, Life & Style':'Others',\n",
        " 'World, Music':'Others',\n",
        " 'Balochistan, K-P':'National',\n",
        " 'Sindh, Technology':'National',\n",
        " 'Film':'Others',\n",
        " 'Sindh, Life & Style, Music':'Others',\n",
        " 'Life & Style, Gossip, TV':'Others',\n",
        " 'Life & Style, Art and Books':'Others',\n",
        " 'K-P, Technology':'National',\n",
        " 'Magazine':'Others',\n",
        " 'Film, Gossip':'Others',\n",
        " 'Life & Style, Theatre':'Others',\n",
        " 'Business, Technology':'Business',\n",
        " 'Balochistan, Gilgit Baltistan':'National',\n",
        " 'K-P, Health':'National',\n",
        " 'Pakistan, Gilgit Baltistan':'National',\n",
        " 'Life & Style, Film, Fashion':'Others',\n",
        " 'Fashion':'Others',\n",
        " 'Punjab, World':'National',\n",
        " 'Pakistan, Sports':'Others',\n",
        " 'Pakistan, Technology':'National',\n",
        " 'Balochistan, Health':'National',\n",
        " 'Pakistan, Sindh, Art and Books':'Others',\n",
        " 'Life & Style, Fashion, TV':'Others',\n",
        " 'World, Health':'International',\n",
        " 'TV, Theatre':'Others',\n",
        " 'Life & Style, Food':'Others',\n",
        " 'Pakistan, Film':'Others',\n",
        " 'Health':'Others',\n",
        " 'World, K-P':'National',\n",
        " 'Life & Style, K-P':'Others',\n",
        " 'Art and Books':'Others',\n",
        " 'Opinion, Health':'Others',\n",
        " 'Life & Style, Music, TV':'Others',\n",
        " 'Sports, Life & Style':'Others',\n",
        " 'Sindh, Jammu & Kashmir':'National',\n",
        " 'Sindh, Business':'Business',\n",
        " 'Life & Style, Film, Theatre':'Others',\n",
        " 'Sindh, Sports':'Others',\n",
        " 'archives':'Others',\n",
        " 'Music, Health':'Others',\n",
        " 'Punjab, Technology':'National',\n",
        " 'Punjab, Health':'National',\n",
        " 'Life & Style, Food, Gossip':'Others',\n",
        " 'Life & Style, Food, Health':'Others',\n",
        " 'Sports, K-P':'Others',\n",
        " 'World, archives':'International',\n",
        " 'Sindh, Life & Style':'Others',\n",
        " 'Life & Style, Music, Theatre':'Others',\n",
        " 'World, Videos':'Others',\n",
        " 'Sports, Videos':'Others',\n",
        " 'Opinion, Technology':'Others',\n",
        " 'Sindh, World':'National',\n",
        " 'Pakistan, K-P, Health':'National',\n",
        " 'Pakistan, Sindh, Punjab':'National',\n",
        " 'Life & Style, Opinion':'Others',\n",
        " 'Balochistan, Life & Style':'Others',\n",
        " 'Life & Style, Art and Books, Film':'Others',\n",
        " 'Pakistan, Opinion':'National',\n",
        " 'Punjab, Jammu & Kashmir':'National',\n",
        " 'World, Opinion':'International',\n",
        " 'Punjab, Sports':'Others',\n",
        " 'Life & Style, Technology':'Others',\n",
        " 'Gossip':'Others',\n",
        " 'Life & Style, Music, Film':'Others',\n",
        " 'Life & Style, Health, TV':'Others',\n",
        " 'Technology, Games':'Others',\n",
        " 'Pakistan, World, Jammu & Kashmir':'National',\n",
        " 'Life & Style, Music, Fashion':'Others',\n",
        " 'Life & Style, Art and Books, Health':'Others',\n",
        " 'Sindh, Videos':'Others',\n",
        " 'Punjab, Food':'Others',\n",
        " 'Life & Style, Film, Health':'Others',\n",
        " 'Sports, Multan, Cities':'Others',\n",
        " 'Music, Fashion':'Others',\n",
        " 'World, Fashion':'Others',\n",
        " 'Videos':'Others',\n",
        " 'Music, Gossip':'Others',\n",
        " 'World, Food, Technology':'International',\n",
        " 'Food, Health':'Others',\n",
        " 'Gossip, TV':'Others',\n",
        " 'Business, Jammu & Kashmir':'Business',\n",
        " 'Sindh, Balochistan':'National',\n",
        " 'Opinion, Editorial':'Editorial',\n",
        " 'Pakistan, archives':'National',\n",
        " 'Jammu & Kashmir, Gilgit Baltistan':'National',\n",
        " 'Punjab, K-P':'National',\n",
        " 'Business, K-P':'Business',\n",
        " 'Life & Style, Fashion, Health':'Others',\n",
        " 'World, Azad Jammu & Kashmir':'International',\n",
        " 'Life &amp; Style, TV':'Others',\n",
        " 'Sindh, Business, Health':'National',\n",
        " 'Sports, Business':'National',\n",
        " 'Punjab, Business, Lahore, Cities':'National',\n",
        " 'Punjab, Life & Style, Gossip':'Others',\n",
        " 'Life & Style, Art and Books, TV':'Others',\n",
        " 'Sindh, Karachi, Cities':'National',\n",
        " 'Sindh, archives':'National',\n",
        " 'Pakistan, Islamabad':'National',\n",
        " 'life and style':'Others',\n",
        " 'life and style, Music':'Others',\n",
        " 'Islamabad':'National',\n",
        " 'Pakistan, Health, Food':'Others',\n",
        " 'Pakistan, Khyber-Pakhtunkhwa':'National',\n",
        " 'Music, Film, Theatre':'Others',\n",
        " 'Music':'Others',\n",
        " 'Cricket':'Others',\n",
        " 'Film, Gossip, Bollywood':'Others',\n",
        " 'Khyber-Pakhtunkhwa':'National',\n",
        " 'Sindh, Karachi':'National',\n",
        " 'Khyber-Pakhtunkhwa, Pakistan':'National',\n",
        " 'World, Azad Jammu & Kashmir, Cities':'National',\n",
        " 'Pakistan, Islamabad, Cities':'National',\n",
        " 'Gossip, TV, Music':'Others',\n",
        " 'Music, Pakistan, Life & Style':'Others',\n",
        " 'Bollywood, Film':'Others',\n",
        " 'Pakistan, Azad Jammu & Kashmir':'National',\n",
        " 'Art and Books, Film, Games':'Others',\n",
        " 'Pakistan, Khyber-Pakhtunkhwa, Cities':'National',\n",
        " 'Sindh, Hyderabad':'National',\n",
        " 'Sindh, Cities':'National',\n",
        " 'Pakistan, Sindh, Cities':'National',\n",
        " 'Art and Books, Film':'Others',\n",
        " 'Pakistan, Lahore':'Others',\n",
        " 'Sports, TV, Gossip':'Others',\n",
        " 'Punjab, Pakistan, Lahore':'National',\n",
        " 'Games':'Others',\n",
        " 'Khyber-Pakhtunkhwa, Swat':'National',\n",
        " 'Pakistan, Sindh, Karachi, Business':'Business',\n",
        " 'Pakistan, Punjab, Khyber-Pakhtunkhwa':'National',\n",
        " 'Football':'Others',\n",
        " 'Pakistan, Peshawar':'National',\n",
        " 'TV, Sports':'Others',\n",
        " 'Khyber-Pakhtunkhwa, Cities, Peshawar':'National',\n",
        " 'Rawalpindi':'National',\n",
        " 'Slideshows, World':'Others',\n",
        " 'Pakistan, Azad Jammu & Kashmir, Cities':'National',\n",
        " 'Pakistan, Cities, Khyber-Pakhtunkhwa':'National',\n",
        " 'Gossip, Film':'Others',\n",
        " 'Health, ADVICE':'Others',\n",
        " 'TV, Film':'Others',\n",
        " 'Pakistan, Lahore, Cities':'National',\n",
        " 'Pakistan, Gwadar, Cities':'National',\n",
        " 'Rawalpindi, Punjab':'National',\n",
        " 'Punjab, Rawalpindi':'National',\n",
        " 'Khyber-Pakhtunkhwa, Peshawar':'National',\n",
        " 'Punjab, Lahore':'National',\n",
        " 'Pakistan, Balochistan, Cities':'National',\n",
        " 'Sindh, khairpur':'National',\n",
        " 'Sports, TV':'Others',\n",
        " 'Islamabad, Pakistan, Cities':'National',\n",
        " 'Pakistan, Khyber-Pakhtunkhwa, Punjab':'National',\n",
        " 'Pakistan, Sindh, Karachi':'National',\n",
        " 'Khyber-Pakhtunkhwa, Abbottabad':'National',\n",
        " 'TV, Gossip':'Others',\n",
        " 'Pakistan, Khyber-Pakhtunkhwa, Gilgit-Baltistan':'National',\n",
        " 'World, Pakistan':'National',\n",
        " 'Khyber-Pakhtunkhwa, Cities':'National',\n",
        " 'Fashion, Gossip':'Others',\n",
        " 'Islamabad, World':'National',\n",
        " 'Punjab, Multan':'National',\n",
        " 'Punjab, Faisalabad':'National',\n",
        " 'Pakistan, Islamabad, Sindh':'National',\n",
        " 'Pakistan, Karachi, Cities':'National',\n",
        " 'Islamabad, Business':'Business',\n",
        " 'Sindh, Pakistan':'National',\n",
        " 'Pakistan, Islamabad, Punjab':'National',\n",
        " 'Pakistan, Sindh, Karachi, Cities':'National',\n",
        " 'Pakistan, Cities, Azad Jammu & Kashmir':'National',\n",
        " 'Health, Life & Style, ADVICE':'Others',\n",
        " 'Islamabad, Balochistan':'National',\n",
        " 'Pakistan, Gilgit-Baltistan':'National',\n",
        " 'Pakistan, Punjab, Cities':'National',\n",
        " 'Sindh, tharparkar':'National',\n",
        " 'Business, Gilgit-Baltistan':'Business',\n",
        " 'Khyber Pakhtunkhwa':'National',\n",
        " 'World, Bollywood':'Others',\n",
        " 'Film, TV, Life & Style, life and style':'Others',\n",
        " 'TV, Film, Life & Style':'Others',\n",
        " 'Film, Life & Style':'Others',\n",
        " 'Pakistan, Cities, Lahore':'National',\n",
        " 'Life & Style, Gossip, Film':'Others',\n",
        " 'Film, Art and Books':'Others',\n",
        " 'Food, ADVICE, Health':'Others',\n",
        " 'Gossip, Fashion':'Others',\n",
        " 'Gossip, Life & Style':'Others',\n",
        " 'Pakistan, Khyber-Pakhtunkhwa, Mardan':'National',\n",
        " 'Pakistan, Punjab, Lahore':'National',\n",
        " 'Life & Style, Gossip, Fashion':'Others',\n",
        " 'Pakistan, Cities':'National',\n",
        " 'Islamabad, Rawalpindi':'National',\n",
        " 'Islamabad, Pakistan':'National',\n",
        " 'Pakistan, Islamabad, World':'National',\n",
        " 'Pakistan, Sindh, Nawabshah':'National',\n",
        " 'Balochistan, Islamabad, Pakistan':'National',\n",
        " 'Fashion, Life & Style':'Others',\n",
        " 'Health, ADVICE, Life & Style':'Others',\n",
        " 'Music, Life & Style':'Others',\n",
        " 'Pakistan, Peshawar, Islamabad, Cities':'National',\n",
        " 'Punjab, Gilgit-Baltistan':'National',\n",
        " 'Health, World':'Others',\n",
        " 'Gossip, Life & Style, Music':'Others',\n",
        " 'Gossip, Music':'Others',\n",
        " 'Gossip, Film, TV':'Others',\n",
        " 'Technology, Life & Style':'Others',\n",
        " 'Sindh, Pakistan, Karachi, Cities':'National',\n",
        " 'Gossip, TV, Film, Life & Style':'Others',\n",
        " 'Pakistan, World, Islamabad, Cities':'National',\n",
        " 'Khyber-Pakhtunkhwa, Mardan':'National',\n",
        " 'Sindh, Nawabshah':'National',\n",
        " 'Azad Jammu & Kashmir, Pakistan':'National',\n",
        " 'TV, Gossip, Life & Style':'Others',\n",
        " 'Film, TV, Life & Style':'Others',\n",
        " 'Khyber-Pakhtunkhwa, Peshawar, Sports':'Others',\n",
        " 'Film, Gossip, Life & Style':'Others',\n",
        " 'Rawalpindi, Islamabad':'National',\n",
        " 'Sindh, sukkur':'National',\n",
        " 'Pakistan, World, Islamabad':'National',\n",
        " 'Pakistan, Sindh, Karachi, Islamabad':'National',\n",
        " 'Business, Pakistan':'Business',\n",
        " 'ADVICE, Life & Style, Film':'Others',\n",
        " 'Film, Life & Style, TV, Gossip':'Others',\n",
        " 'Music, TV':'Others',\n",
        " 'Pakistan, Azad Jammu & Kashmir, Islamabad':'National',\n",
        " 'Music, Gossip, Life & Style':'Others',\n",
        " 'Islamabad, Cities, Pakistan':'National',\n",
        " 'Islamabad, Punjab':'National',\n",
        " 'World, Business':'Business',\n",
        " 'Health, Life & Style':'Others',\n",
        " 'Technology, World':'Others',\n",
        " 'Islamabad, Khyber-Pakhtunkhwa':'National',\n",
        " 'Pakistan, Sindh, Karachi, Hyderabad':'National',\n",
        " 'Life & Style, TV, Gossip':'Others',\n",
        " 'Khyber Pakhtunkhwa, Mardan':'National',\n",
        " 'ADVICE, Life & Style':'Others',\n",
        " 'Pakistan, Sindh, Balochistan':'National',\n",
        " 'Gossip, Life & Style, TV':'Others',\n",
        " 'Music, Film, Gossip':'Others',\n",
        " 'Khyber Pakhtunkhwa, Islamabad':'National',\n",
        " 'Pakistan, World, Azad Jammu & Kashmir':'National',\n",
        " 'World, Khyber Pakhtunkhwa, Peshawar, Islamabad, Pakistan':'National',\n",
        " 'Rawalpindi, Pakistan, Business':'Business',\n",
        " 'Khyber Pakhtunkhwa, Peshawar':'National',\n",
        " 'Sindh, dadu':'National',\n",
        " 'Fashion, Gossip, Life & Style':'Others',\n",
        " 'Pakistan, Karachi, Sindh':'National',\n",
        " 'Pakistan, Balochistan, Quetta':'National',\n",
        " 'TV, Life & Style':'Others',\n",
        " 'Pakistan, Islamabad, World, Cities':'National',\n",
        " 'Pakistan, Khyber Pakhtunkhwa, Cities':'National',\n",
        " 'Pakistan, Karachi, Sindh, Cities':'National',\n",
        " 'Pakistan, Rawalpindi':'National',\n",
        " 'Punjab, Film':'Others',\n",
        " 'Khyber Pakhtunkhwa, Swat':'National',\n",
        " 'Balochistan, Gwadar':'National',\n",
        " 'TV, Life & Style, Gossip':'Others',\n",
        " 'Sindh, Karachi, Pakistan, Cities':'National',\n",
        " 'Pakistan, Sindh, Islamabad':'National',\n",
        " 'Pakistan, Punjab, Islamabad, Cities':'National',\n",
        " 'Khyber Pakhtunkhwa, Nowshera':'National',\n",
        " 'Balochistan, Quetta':'National',\n",
        " 'TV, Film, Gossip':'Others',\n",
        " 'Abbottabad':'National',\n",
        " 'Pakistan, Karachi, Peshawar':'National',\n",
        " 'Pakistan, Khyber Pakhtunkhwa':'National',\n",
        " 'Sports, Hockey':'Others',\n",
        " 'Art and Books, Life & Style':'Others',\n",
        " 'Pakistan, World, Balochistan':'National',\n",
        " 'Theatre':'Others',\n",
        " 'Gossip, Film, Life & Style':'Others',\n",
        " 'Pakistan, Business, Khyber Pakhtunkhwa':'Business',\n",
        " 'Film, TV, Gossip, Life & Style':'Others',\n",
        " 'Life & Style, Sports':'Others',\n",
        " 'Music, Art and Books, Life & Style, Film':'Others',\n",
        " 'Pakistan, Sindh, Karachi, Hyderabad, Cities':'National',\n",
        " 'Life & Style, ADVICE':'Others',\n",
        " 'Pakistan, Sindh, Punjab, Azad Jammu & Kashmir, Gilgit-Baltistan, Khyber Pakhtunkhwa, Balochistan':'National',\n",
        " 'Pakistan, Karachi':'National',\n",
        " 'Fashion, Music':'Others',\n",
        " 'Film, TV':'Others',\n",
        " 'Pakistan, Khyber Pakhtunkhwa, Mansehra, Nowshera, Charsadda':'National',\n",
        " 'Fashion, Gossip, TV':'Others',\n",
        " 'Karachi, Pakistan':'National',\n",
        " 'Pakistan, Azad Jammu & Kashmir, Muzaffarabad':'National',\n",
        " 'Pakistan, Azad Jammu & Kashmir, World':'National',\n",
        " 'Film, Music, Gossip':'Others',\n",
        " 'Pakistan, Punjab, Rawalpindi':'National',\n",
        " 'Pakistan, Lahore, Punjab':'National',\n",
        " 'World, Newslab':'International',\n",
        " 'Karachi':'National',\n",
        " 'Pakistan, Islamabad, Azad Jammu & Kashmir':'National',\n",
        " 'Gossip, Fashion, Life & Style':'Others',\n",
        " 'Pakistan, Rawalpindi, Karachi':'National',\n",
        " 'Art and Books, Gossip, Life & Style':'Others',\n",
        " 'Film, Theatre, Life & Style':'Others',\n",
        " 'Pakistan, Punjab, Islamabad':'National',\n",
        " 'Gilgit-Baltistan, Pakistan':'National',\n",
        " 'Peshawar, Khyber Pakhtunkhwa, Pakistan':'National',\n",
        " 'Khyber Pakhtunkhwa, Pakistan, Cities':'National',\n",
        " 'Life & Style, TV, Film':'Others',\n",
        " 'Punjab, Pakistan':'National',\n",
        " 'Business, Sindh, Karachi':'Business',\n",
        " 'Lahore, Pakistan':'National',\n",
        " 'Pakistan, World, Business':'Business',\n",
        " 'Life & Style, Bollywood, Art and Books':'Others',\n",
        " 'TV, Fashion':'Others',\n",
        " 'Khyber Pakhtunkhwa, Pakistan, Peshawar':'National',\n",
        " 'Pakistan, Punjab, Faisalabad':'National',\n",
        " 'Life & Style, Bollywood':'Others',\n",
        " 'Pakistan, Punjab, Gujranwala':'National',\n",
        " 'Pakistan, Khyber Pakhtunkhwa, Peshawar':'National',\n",
        " 'Gossip, Bollywood':'Others',\n",
        " 'Khyber Pakhtunkhwa, Pakistan, Charsadda':'National',\n",
        " 'Pakistan, World, Technology':'International',\n",
        " 'Pakistan, Balochistan, Khyber Pakhtunkhwa':'National',\n",
        " 'Gilgit-Baltistan, Pakistan, Skardu':'National',\n",
        " 'Punjab, Pakistan, Gujranwala':'National',\n",
        " 'Sindh, Pakistan, Karachi':'National',\n",
        " 'Balochistan, Pakistan':'National',\n",
        " 'Pakistan, Sindh, sukkur':'National',\n",
        " 'T.Edit':'Others',\n",
        "'Fashion, Film':'Others',\n",
        "'Pakistan, Azad Jammu & Kashmir, Rawalpindi':'National',\n",
        "'Pakistan, Punjab, Multan':'National',\n",
        "'Khyber Pakhtunkhwa, Pakistan':'National',\n",
        "'Gilgit-Baltistan':'National',\n",
        "'Pakistan, Islamabad, Rawalpindi':'National',\n",
        "'Pakistan, Khyber Pakhtunkhwa, Mansehra':'National',\n",
        "'World, Pakistan, Azad Jammu & Kashmir':'National',\n",
        "'Pakistan, Gilgit-Baltistan, gilgit':'National',\n",
        "'Sindh, Karachi, Pakistan':'National',\n",
        "'Pakistan, Khyber Pakhtunkhwa, Punjab':'National',\n",
        "'Pakistan, Khyber Pakhtunkhwa, Swat':'National',\n",
        "'Pakistan, Islamabad, Karachi':'National',\n",
        "'Pakistan, Sindh, Balochistan, Karachi':'National',\n",
        "'Pakistan, Islamabad, Lahore':'National',\n",
        "'Food, Life & Style':'Others',\n",
        "'Film, Music':'Others',\n",
        "'Pakistan, POLITICS':'National',\n",
        "'Pakistan, World, Multan, Punjab':'National',\n",
        "'Pakistan, Sindh, Balochistan, Islamabad, Khyber Pakhtunkhwa, Punjab':'National',\n",
        "'Pakistan, Sindh, Punjab, Balochistan, Khyber Pakhtunkhwa':'National',\n",
        "'Art and Books, Fashion':'Others',\n",
        "'Pakistan, Gilgit-Baltistan, Abbottabad':'National',\n",
        "'World, Pakistan, Islamabad':'National',\n",
        "'Pakistan, Punjab, Khyber Pakhtunkhwa, Bannu, Dera Ismail Khan':'National',\n",
        "'Pakistan, Islamabad, Khyber Pakhtunkhwa':'National',\n",
        "'Pakistan, Sindh, Karachi, Quetta, Balochistan':'National',\n",
        "'Life & Style, Business':'Others',\n",
        "'Pakistan, Punjab, Lahore, Islamabad':'National',\n",
        "'Punjab, Pakistan, Rawalpindi':'National',\n",
        "'Hockey':'Others',\n",
        "'Pakistan, Sindh, khairpur':'National',\n",
        "'Pakistan, Islamabad, Sindh, Punjab, Balochistan, Khyber Pakhtunkhwa':'National',\n",
        "'Punjab, Lahore, Pakistan':'National',\n",
        "'Khyber Pakhtunkhwa, Pakistan, Swat':'National',\n",
        "'Pakistan, Punjab, Sindh, Balochistan, Islamabad, Gilgit-Baltistan, Azad Jammu & Kashmir':'National',\n",
        "'World, Pakistan, Balochistan':'National',\n",
        "'Pakistan, Quetta':'National',\n",
        "'Multan, Pakistan, Punjab':'National',\n",
        "'Pakistan, Cricket':'Others',\n",
        "'Gossip, Art and Books':'Others',\n",
        "'Karachi, Pakistan, Islamabad':'National',\n",
        "'Sindh, Pakistan, Cities':'National',\n",
        "'Karachi, Sindh':'National',\n",
        "'Pakistan, Rawalpindi, Punjab':'National',\n",
        "'Islamabad, Pakistan, Punjab':'National',\n",
        "'Lahore':'National',\n",
        "'Azad Jammu & Kashmir':'National',\n",
        "'Islamabad, Peshawar':'National',\n",
        "'Pakistan, Islamabad, Punjab, Khyber Pakhtunkhwa, Balochistan':'National',\n",
        "'Pakistan, Islamabad, Lahore, Punjab':'National',\n",
        "'Sindh, Pakistan, sukkur':'National',\n",
        "'Khyber Pakhtunkhwa, Peshawar, Pakistan':'National',\n",
        "'Pakistan, Punjab, Sindh':'National',\n",
        "'Islamabad, Karachi, Lahore, Pakistan':'National',\n",
        "'Karachi, Pakistan, Sindh':'National',\n",
        "'Pakistan, Peshawar, Khyber Pakhtunkhwa':'National',\n",
        "'Pakistan, Punjab, Islamabad, Khyber Pakhtunkhwa, Balochistan, Azad Jammu & Kashmir, Gilgit-Baltistan, Sindh':'National',\n",
        "'Islamabad, Rawalpindi, Pakistan':'National',\n",
        "'Khyber Pakhtunkhwa, Gilgit-Baltistan':'National',\n",
        "'Pakistan, Khyber Pakhtunkhwa, Nowshera':'National',\n",
        "'Quetta, Pakistan, Balochistan':'National',\n",
        "'Pakistan, Sindh, Karachi, Punjab, Lahore':'National',\n",
        "'Life & Style, World':'Others',\n",
        "'Pakistan, Islamabad, Business':'Business',\n",
        "'Islamabad, Khyber Pakhtunkhwa':'National',\n",
        "'Pakistan, Punjab, Khyber Pakhtunkhwa':'National',\n",
        "'Pakistan, Quetta, Balochistan':'National',\n",
        "'Pakistan, Islamabad, Mirpur':'National',\n",
        "'Pakistan, Punjab, Khyber Pakhtunkhwa, Sindh':'National',\n",
        "'Health, Latest':'Others',\n",
        "'Pakistan, Balochistan, Zhob':'National',\n",
        "'Technology, Business':'Business',\n",
        "'Lahore, Punjab':'National',\n",
        "'Pakistan, Muzaffarabad':'National',\n",
        "'Pakistan, Sindh, Opinion':'National',\n",
        "'Pakistan, Khyber Pakhtunkhwa, Sindh':'National',\n",
        "'Pakistan, Karachi, World':'National',\n",
        "'Technology, Business, World':'Business',\n",
        "'Pakistan, Islamabad, Gilgit-Baltistan':'National',\n",
        "'Peshawar, Khyber Pakhtunkhwa':'National',\n",
        "'Khyber Pakhtunkhwa, Pakistan, Islamabad':'National',\n",
        "'Pakistan, Sindh, Life & Style':'Others',\n",
        "'Pakistan, Sindh, Hyderabad':'National',\n",
        "'gilgit':'National',\n",
        "'Sports, Tennis':'Others',\n",
        "'Sports, Football':'Others',\n",
        "'Pakistan, Gilgit-Baltistan, Islamabad':'National',\n",
        "'TV, Music, Fashion':'Others',\n",
        "'Pakistan, Business, Islamabad':'Business',\n",
        "'Pakistan, Karachi, Sindh, Balochistan':'National',\n",
        "'Fashion, TV, Music':'Others',\n",
        "'World, Pakistan, Sports':'Others',\n",
        "'Sports, World':'Others',\n",
        "'Pakistan, Sports, Cricket':'Others',\n",
        "'Sports, Pakistan':'Others',\n",
        "'Technology, Pakistan':'Others',\n",
        "'Pakistan, gilgit':'National',\n",
        "'Film, Fashion':'Others',\n",
        "'TV, Film, Music':'Others',\n",
        "'TV, Music':'Others',\n",
        "'Film, TV, Art and Books':'Others',\n",
        "'Karachi, Life & Style':'Others',\n",
        "'Music, Film, TV':'Others',\n",
        "'Azad Jammu & Kashmir, Khyber Pakhtunkhwa':'National',\n",
        "'POLITICS, World':'International',\n",
        "'Pakistan, Punjab, Opinion':'National',\n",
        "'Pakistan, Gwadar, Balochistan':'National',\n",
        "'POLITICS':'National',\n",
        "'Film, Music, TV, Art and Books':'Others',\n",
        "'Gossip, TV, Film':'Others',\n",
        "'Multimedia':'Others',\n",
        "'Azad Jammu & Kashmir, World':'International',\n",
        "'Pakistan, Technology, Sindh, Karachi':'National',\n",
        "'Pakistan, Sindh, tharparkar':'National',\n",
        "'Pakistan, Sindh, Sanghar':'National',\n",
        "'Fashion, TV':'Others',\n",
        "'Trends':'Others',\n",
        "'POLITICS, Pakistan':'National',\n",
        "'Life & Style, Spotlight':'Others',\n",
        "'Spotlight':'Others',\n",
        "'Film, Art and Books, TV, Music':'Others',\n",
        "'POLITICS, Life & Style':'Others',\n",
        "'Spotlight, Music':'Others',\n",
        "'Music, Spotlight':'Others',\n",
        "'Gossip, Spotlight':'Others',\n",
        "'Spotlight, TV':'Others',\n",
        "'Spotlight, Gossip':'Others',\n",
        "'Pakistan, Khyber Pakhtunkhwa, Mardan':'National',\n",
        "'Spotlight, Film':'Others',\n",
        "'Health, Spotlight':'Others',\n",
        "'Spotlight, Fashion':'Others',\n",
        "'Technology, Food':'Others',\n",
        "'Art and Books, Spotlight':'Others',\n",
        "'Pakistan, Balochistan, Derabugti':'National',\n",
        "'Sports, Pakistan, Cricket':'Others',\n",
        "'Pakistan, Balochistan, Gwadar':'National',\n",
        "'Pakistan, Life & Style, MOVIES':'Others',\n",
        "'Film, Spotlight':'Others',\n",
        "'Business, Life & Style':'Business',\n",
        "'Pakistan, Cricket, Sports':'Others',\n",
        "'Pakistan, Balochistan, Football, Gwadar':'Others',\n",
        "'Pakistan, Sindh, Thatta':'National',\n",
        "'Karachi, Sindh, Pakistan':'National',\n",
        "'Pakistan, Faisalabad, Punjab':'National',\n",
        "'TV, Spotlight':'Others',\n",
        "'Technology, Health':'Others',\n",
        "'Health, Technology':'Others',\n",
        "'Pakistan, Swat, Khyber Pakhtunkhwa':'National',\n",
        "'Sports, Technology':'Others',\n",
        "'Pakistan, Hyderabad, Sindh':'National',\n",
        "'Spotlight, Food':'Others',\n",
        "'Bollywood, Gossip':'Others',\n",
        "'Technology, Sports':'Others',\n",
        "'Pakistan, Larkana':'National',\n",
        "'Pakistan, Jacobabad':'National',\n",
        "'Art and Books, Theatre':'Others',\n",
        "'World, Azad Jammu & Kashmir, Pakistan':'National',\n",
        "'Sports, Spotlight':'Others',\n",
        "'Spotlight, Sports':'Others',\n",
        "'Latest':'Others',\n",
        "'Punjab, Islamabad':'National',\n",
        "'Tennis':'Others',\n",
        "'Football, Sports':'Others',\n",
        "'Pakistan, Business, World':'Business',\n",
        "'Pakistan, Business, Life & Style':'Business',\n",
        "'Fashion, Spotlight':'Others',\n",
        "    }\n",
        "}\n",
        "# Function to map categories based on the category_mappings dictionary\n",
        "def map_categories(row):\n",
        "    source = row['source']\n",
        "    category = row['categories']\n",
        "\n",
        "    if source in category_mappings and category in category_mappings[source]:\n",
        "        return category_mappings[source][category]\n",
        "    else:\n",
        "        # Increment a counter when 'Others' is returned\n",
        "        global others_count\n",
        "        others_count += 1\n",
        "        print(f'Category mapping not found for row: Source={source}, Category={category}')\n",
        "        #return category  # Use the original category if not found in mapping\n",
        "        return 'Others'  # Append 'Others' to the 'main_cat' column if not found in mapping\n",
        "\n",
        "\n",
        "# Initialize the counter\n",
        "others_count = 0\n",
        "\n",
        "# Apply the mapping function to the DataFrame\n",
        "merged_df['main_cat'] = merged_df.apply(map_categories, axis=1)\n",
        "\n",
        "# Print the count of 'Others'\n",
        "print(f'Total number of rows with \"Others\": {others_count}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GhQFBUUS8mP"
      },
      "outputs": [],
      "source": [
        "# Get unique main categories\n",
        "main_categories = merged['main_cat'].unique()\n",
        "print(main_categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8CjUTG6SoXi"
      },
      "source": [
        "# Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select Only Business News, World News, National News"
      ],
      "metadata": {
        "id": "vHyvqomkSpvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df[merged_df['main_cat'] != 'Others']\n",
        "print(merged_df.shape)"
      ],
      "metadata": {
        "id": "EqZTIJLRSvvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_categories = merged_df['main_cat'].unique()\n",
        "print(main_categories)"
      ],
      "metadata": {
        "id": "DyNR5GRdFKdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEhZ6EHRSo9e"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespaces\n",
        "\n",
        "    # Remove \"Published in Dawn\" with a regular expression\n",
        "    text = re.sub(r'Published in Dawn, [A-Za-z]+\\s\\d{1,2}(st|nd|rd|th)?,\\s\\d{4}', '', text)\n",
        "\n",
        "    # Remove \"Published in Dawn\" with a regular expression\n",
        "    text = re.sub(r'published in dawn, [A-Za-z]+\\s\\d{1,2}(st|nd|rd|th)?,\\s\\d{4}', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove \"(adsbygoogle = window.adsbygoogle || [ ] ) .push ( { } )\"\n",
        "    text = re.sub(r'\\(adsbygoogle\\s*=\\s*window.adsbygoogle\\s*\\|\\|\\s*\\[\\s*\\]\\s*\\)\\s*\\.\\s*push\\s*\\(\\s*\\{\\s*\\}\\s*\\)', '', text)\n",
        "\n",
        "    # Remove copyright notice with a regular expression\n",
        "    text = re.sub(r'copyright business recorder, \\d{4}', '', text)\n",
        "\n",
        "    # Remove copyright notice with a regular expression\n",
        "    text = re.sub(r'Copyright Business Recorder, \\d{4}', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Lemmatize the tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Stem the tokens\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Join the tokens back into a single string\n",
        "    cleaned_text = ' '.join(tokens)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "def text_for_display(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespaces\n",
        "    #text = text.lower()\n",
        "    # Remove \"Published in Dawn\" with a regular expression\n",
        "    text = re.sub(r'Published in Dawn, [A-Za-z]+\\s\\d{1,2}(st|nd|rd|th)?,\\s\\d{4}', '', text)\n",
        "    text = re.sub(r'\\(adsbygoogle\\s*=\\s*window.adsbygoogle\\s*\\|\\|\\s*\\[\\s*\\]\\s*\\)\\s*\\.\\s*push\\s*\\(\\s*\\{\\s*\\}\\s*\\)', '', text)\n",
        "    text = re.sub(r'Copyright Business Recorder, \\d{4}', '', text)\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))  # Set of stopwords for English language\n",
        "    tokens = word_tokenize(text)  # Tokenize the text into individual words\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]  # Filter out stopwords\n",
        "    filtered_text = ' '.join(filtered_tokens)  # Join the filtered words back into a single string\n",
        "    return filtered_text\n",
        "\n",
        "def remove_punc(text):\n",
        "     #text = re.sub(r'[^\\w\\s.]', '', text)  #Remove punctuation marks other than .\n",
        "     text = re.sub(r'[^\\w\\s]', '', text)  # Remove all punctuation marks, including periods\n",
        "     return text\n",
        "\n",
        "def extract_first_15_sentences(text):\n",
        "    if text is not None:\n",
        "        sentences = sent_tokenize(text)\n",
        "        first_15_sentences = ' '.join(sentences[:15])\n",
        "        return first_15_sentences\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def clean_wo_stemming_tokenization(text):\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespaces\n",
        "\n",
        "    # Remove \"Published in Dawn\" with a regular expression\n",
        "    text = re.sub(r'Published in Dawn, [A-Za-z]+\\s\\d{1,2}(st|nd|rd|th)?,\\s\\d{4}', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove \"(adsbygoogle = window.adsbygoogle || [ ] ) .push ( { } )\"\n",
        "    text = re.sub(r'\\(adsbygoogle\\s*=\\s*window.adsbygoogle\\s*\\|\\|\\s*\\[\\s*\\]\\s*\\)\\s*\\.\\s*push\\s*\\(\\s*\\{\\s*\\}\\s*\\)', '', text)\n",
        "\n",
        "    # Remove copyright notice with a regular expression\n",
        "    text = re.sub(r'copyright business recorder, \\d{4}', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Join the tokens back into a single string\n",
        "    cleaned_text = ' '.join(tokens)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "merged_df['Combined'] = merged_df['Headline'] + ' ' + merged_df['News']\n",
        "\n",
        "merged_df ['Combined_Cleaned']= merged_df['Combined'].apply(clean_text)\n",
        "\n",
        "merged_df ['CC_wo_Stopwords']= merged_df['Combined_Cleaned'].apply(remove_stopwords)\n",
        "\n",
        "merged_df['CC_wo_Stopwords_Punc']= merged_df ['CC_wo_Stopwords'].apply(remove_punc)\n",
        "\n",
        "merged_df['CC_wo_Stem_Lema_Punc']= merged_df ['Combined'].apply(text_for_display)\n",
        "\n",
        "merged_df ['CC_wo_Stem_Lema_Punc']= merged_df ['CC_wo_Stem_Lema_Punc'].apply(remove_punc)\n",
        "\n",
        "merged_df['Cleaned_description']= merged_df['News'].apply(text_for_display)\n",
        "\n",
        "\n",
        "print(merged_df.info())\n",
        "print(merged_df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hKK8WzGSqvc"
      },
      "outputs": [],
      "source": [
        "# Convert the 'CC_wo_Stem_Lema_Punc' column to lowercase\n",
        "merged_df['CC_wo_Stem_Lema_Punc'] = merged_df['CC_wo_Stem_Lema_Punc'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a variable to keep track of the maximum length\n",
        "max_length = 0\n",
        "\n",
        "# Iterate through the rows and find the maximum length\n",
        "for text in filtered_df['Combined_Cleaned']:\n",
        "    length = len(text)\n",
        "    if length > max_length:\n",
        "        max_length = length\n",
        "\n",
        "# Print the maximum length\n",
        "print(\"Maximum Length of 'Combined_Cleaned':\", max_length)"
      ],
      "metadata": {
        "id": "VFXPCrUNVQe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAfLJ6H3ETpN"
      },
      "source": [
        "# Renaming Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvDMvuVuEW7n"
      },
      "outputs": [],
      "source": [
        "column_mapping = {\n",
        "    'headline': 'Headline',\n",
        "    'date': 'DateTime',\n",
        "    'link': 'Url',\n",
        "    'source': 'Source',\n",
        "    'categories': 'Category',\n",
        "    'description': 'News'\n",
        "}\n",
        "\n",
        "merged = merged.rename(columns=column_mapping)\n",
        "print(merged.shape)\n",
        "merged.to_csv('updated.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeXjUDTlFc9Q"
      },
      "outputs": [],
      "source": [
        "column_mapping = {\n",
        "    'Source': 'source'\n",
        "}\n",
        "\n",
        "merged = merged.rename(columns=column_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SsL_3NyGE5_"
      },
      "outputs": [],
      "source": [
        "column_mapping = {\n",
        "    'Sentiment_Prediction': 'sentiment'\n",
        "}\n",
        "merged = merged.rename(columns=column_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deciding sub-cat of News based upon keyword list"
      ],
      "metadata": {
        "id": "P6LK1jAKRWgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine single-word and double-word keywords, symbols, and company names for the Oil & Gas sector\n",
        "oil_gas_keywords = [\n",
        "    \"Crude Oil\",\n",
        "    \"Natural Gas\",\n",
        "    \"Exploration\",\n",
        "    \"Drilling\",\n",
        "    \"Reservoir\",\n",
        "    \"Refining\",\n",
        "    \"Petrochemical\",\n",
        "    \"Upstream\",\n",
        "    \"Downstream\",\n",
        "    \"Midstream\",\n",
        "    \"Offshore\",\n",
        "    \"Onshore\",\n",
        "    \"Oilfield Services\",\n",
        "    \"Exploration and Production (E&P)\",\n",
        "    \"Refinery\",\n",
        "    \"Exploration and Production Companies\",\n",
        "    \"Oil Price\",\n",
        "    \"Gas Price\",\n",
        "    \"Oilfield Technology\",\n",
        "    \"Energy Sector\",\n",
        "    \"Energy Stocks\",\n",
        "    \"Commodity Prices\",\n",
        "    \"Energy Demand\",\n",
        "    \"OPEC (Organization of the Petroleum Exporting Countries)\",\n",
        "    \"Shale Oil\",\n",
        "    \"Shale Gas\",\n",
        "    \"Reserves\",\n",
        "    \"Crude Oil Inventories\",\n",
        "    \"Natural Gas Storage\",\n",
        "    \"Energy Infrastructure\",\n",
        "    \"Energy Supply Chain\",\n",
        "    \"Oil Rig\",\n",
        "    \"Gas Pipeline\",\n",
        "    \"Energy Exploration Contracts\",\n",
        "    \"Energy Market Trends\",\n",
        "    \"Energy Investments\",\n",
        "    \"Energy Policy\",\n",
        "    \"Energy Regulations\",\n",
        "    \"Renewable Energy Competition\",\n",
        "    \"MARI Mari Petroleum Company Limited\",\n",
        "    \"OGDC Oil & Gas Development Company Limited\",\n",
        "    \"POL Pakistan Oilfields Limited\",\n",
        "    \"PPL Pakistan Petroleum Limited\",\n",
        "    \"APL Attock Petroleum Limited\",\n",
        "    \"BPL Burshane LPG (Pakistan) Limited\",\n",
        "    \"HASCOLDEF Hascol Petroleum Limited\",\n",
        "    \"HTL Hi-Tech Lubricants Limited\",\n",
        "    \"OBOY Oilboy Energy Limited\",\n",
        "    \"PSO Pakistan State Oil Company Limited\",\n",
        "    \"SHEL Shell Pakistan Limited\",\n",
        "    \"SNGP Sui Northern Gas Pipelines Limited\",\n",
        "    \"SSGC Sui Southern Gas Company Limited\",\n",
        "    \"Carbon Emissions\",\n",
        "    \"Cubic Feet\",\n",
        "    \"Exploration and Production\",\n",
        "    \"Well Integrity\",\n",
        "    \"Sui Southern Gas Company\",\n",
        "    \"Public\",\n",
        "    \"Reserve\",\n",
        "    \"Burshane LPG\",\n",
        "    \"Total Parco\",\n",
        "    \"Sui Northern Gas Pipelines Limited\",\n",
        "    \"Basic Power\",\n",
        "    \"Refinery\",\n",
        "    \"Oil & Gas Development Company\",\n",
        "    \"Reserves\",\n",
        "    \"Barrel\",\n",
        "    \"LPG Association\",\n",
        "    \"Offshore Drilling\",\n",
        "    \"Khyber Pakhtunkhwa Oil & Gas Company (KPOGCL)\",\n",
        "    \"Oil Reserves\",\n",
        "    \"Pakistan Refinery Limited (PRL)\",\n",
        "    \"High-Tech Lubricants Limited (HTL)\",\n",
        "    \"Crude Oil\",\n",
        "    \"Zeba\",\n",
        "    \"Hydraulic Fracturing\",\n",
        "    \"Drill Bit\",\n",
        "    \"Energy Mix\",\n",
        "    \"Pipelines\",\n",
        "    \"LNG\",\n",
        "    \"Carbon Storage\",\n",
        "    \"Mari Petroleum\",\n",
        "    \"Kannauj\",\n",
        "    \"Pakistan Petroleum Limited (PPL)\",\n",
        "    \"Commercial Cylinder\",\n",
        "    \"Sui Northern Gas Pipelines Limited\",\n",
        "    \"Pakistan Refinery Limited (PRL)\",\n",
        "    \"Oil Production\",\n",
        "    \"Shell Oil and Gas\",\n",
        "    \"Liquefied Natural Gas\",\n",
        "    \"Energy Security\",\n",
        "    \"Attock Refinery Limited\",\n",
        "    \"Engro Elengy Terminal\",\n",
        "    \"Sui Southern\",\n",
        "    \"Platform\",\n",
        "    \"Oilfield Services\",\n",
        "    \"Exploration and Production (E&P)\",\n",
        "    \"Reservoir\",\n",
        "    \"Crude Oil Inventories\",\n",
        "    \"Liquefied Natural Gas\",\n",
        "    \"LNG\",\n",
        "    \"Exploration\",\n",
        "    \"Exploration and Production\",\n",
        "    \"Carbon Capture and Storage\",\n",
        "    \"Pakistan State Oil\",\n",
        "    \"Pakistan Petroleum Limited\",\n",
        "    \"Well Integrity\",\n",
        "    \"Sui Southern Gas Company\",\n",
        "    \"Public\",\n",
        "    \"Reserve\",\n",
        "    \"Burshane LPG\",\n",
        "    \"Total Parco\",\n",
        "    \"Sui Northern Gas Pipelines Limited\",\n",
        "    \"Basic Power\",\n",
        "    \"Refinery\",\n",
        "    \"Oil & Gas Development Company\",\n",
        "    \"Reserves\",\n",
        "    \"Barrel\",\n",
        "    \"LPG Association\",\n",
        "    \"Offshore Drilling\",\n",
        "    \"KPOGCL\",\n",
        "    \"Oil Reserves\",\n",
        "    \"Pakistan Refinery Limited\",\n",
        "    \"High-Tech Lubricants Limited (HTL)\",\n",
        "    \"Crude Oil\",\n",
        "    \"Zeba\",\n",
        "    \"Hydraulic Fracturing\",\n",
        "    \"Drill Bit\",\n",
        "    \"Energy Mix\",\n",
        "    \"Pipelines\",\n",
        "    \"LNG\",\n",
        "    \"Carbon Storage\",\n",
        "    \"Mari Petroleum\",\n",
        "    \"Kannauj\",\n",
        "    \"PPL\",\n",
        "    \"Commercial Cylinder\",\n",
        "    \"Sui Northern Gas Pipelines Limited\",\n",
        "    \"Pakistan Refinery Limited\",\n",
        "    \"Oil Production\",\n",
        "    \"Shell Oil and Gas\",\n",
        "    \"Liquefied Natural Gas\",\n",
        "    \"Energy Security\",\n",
        "    \"Attock Refinery Limited\",\n",
        "    \"Engro Elengy Terminal\",\n",
        "    \"Sui Southern\",\n",
        "    \"Platform\",\n",
        "    \"Oilfield Services\",\n",
        "    \"Exploration and Production (E&P)\",\n",
        "    \"Reservoir\",\n",
        "    \"Crude Oil Inventories\",\n",
        "    \"Liquefied Natural Gas\",\n",
        "    \"LNG\",\n",
        "    \"Exploration\",\n",
        "    \"Exploration and Production\",\n",
        "    \"Carbon Capture and Storage\",\n",
        "    \"Pakistan State Oil\",\n",
        "    \"Pakistan Petroleum Limited\",\n",
        "    \"Well Integrity\",\n",
        "    \"Sui Southern Gas Company\",\n",
        "    \"Public\",\n",
        "    \"Reserve\",\n",
        "    \"Burshane LPG\",\n",
        "    \"Total Parco\",\n",
        "    \"Sui Northern Gas Pipelines Limited\",\n",
        "    \"Basic Power\",\n",
        "    \"Refinery\",\n",
        "    \"Oil & Gas Development Company\",\n",
        "    \"Reserves\",\n",
        "    \"Barrel\",\n",
        "    \"LPG Association\",\n",
        "    \"Offshore Drilling\",\n",
        "    \"KPOGCL\",\n",
        "    \"Oil Reserves\",\n",
        "    \"Pakistan Refinery Limited\",\n",
        "    \"High-Tech Lubricants Limited (HTL)\",\n",
        "    \"Crude Oil\",\n",
        "    \"Zeba\",\n",
        "    \"Hydraulic Fracturing\",\n",
        "    \"Drill Bit\",\n",
        "    \"Energy Mix\",\n",
        "    \"Pipelines\",\n",
        "    \"LNG\",\n",
        "    \"Carbon Storage\",\n",
        "    \"Mari Petroleum\",\n",
        "    \"Kannauj\",\n",
        "    \"PPL\",\n",
        "    \"Commercial Cylinder\",\n",
        "    \"Sui Northern Gas Pipelines Limited\",\n",
        "    \"Pakistan Refinery Limited\",\n",
        "    \"Oil Production\",\n",
        "    \"Shell Oil and Gas\",\n",
        "    \"Liquefied Natural Gas\",\n",
        "    \"Energy Security\",\n",
        "    \"Attock Refinery Limited\",\n",
        "    \"Engro Elengy Terminal\",\n",
        "    \"Sui Southern\",\n",
        "    \"Platform\",\n",
        "    \"Oilboy Energy Limited\",\n",
        "    \"Hascol Petroleum\",\n",
        "    \"Pakistan Oilfields Limited\",\n",
        "    \"Puma Energy Pakistan\",\n",
        "    \"Total Parco\",\n",
        "    \"Attock Petroleum Limited\",\n",
        "    \"Cubic Feet\",\n",
        "    \"Natural Gas\",\n",
        "    \"Downstream\",\n",
        "    \"OBOY\",\n",
        "    \"Cenergyico\",\n",
        "    \"Sui Southern Gas Company\",\n",
        "    \"Bone\",\n",
        "    \"Coastal\",\n",
        "    \"Hascol Petroleum\",\n",
        "    \"Petroleum Products\",\n",
        "    \"Diesel\",\n",
        "    \"United Energy Pakistan\",\n",
        "    \"Pipeline Installation\",\n",
        "    \"Pak-Arab Refinery\",\n",
        "    \"Cubic Feet\",\n",
        "    \"Natural Gas\",\n",
        "    \"Downstream\",\n",
        "    \"OBOY\",\n",
        "    \"Cenergyico\",\n",
        "    \"NGLs\",\n",
        "    \"Emissions Reduction\",\n",
        "    \"SSGC\",\n",
        "    \"Bone Crystallization\",\n",
        "    \"Mud Logging\",\n",
        "    \"Ministry of Energy\",\n",
        "    \"Boring\",\n",
        "    \"Pakistan Petroleum\",\n",
        "    \"Refining\",\n",
        "    \"Cylinder\",\n",
        "    \"Better Oil Recovery\",\n",
        "    \"Fracking\",\n",
        "    \"Cenergyico\",\n",
        "    \"Crude Oil\",\n",
        "    \"Vein\",\n",
        "    \"Nishpa Drilling Lease\",\n",
        "    \"Production\",\n",
        "    \"Fuel Extraction in Pakistan\",\n",
        "    \"Attock Refinery Limited\",\n",
        "    \"Oil & Gas Development Company\",\n",
        "    \"Pakistan Refinery Limited\"\n",
        "]\n",
        "# Combine single-word and double-word keywords, symbols, and company names\n",
        "cement_keywords = [\n",
        "    \"Cement\",\n",
        "    \"Cement Manufacturing\",\n",
        "    \"Concrete\",\n",
        "    \"Construction Materials\",\n",
        "    \"Building Materials\",\n",
        "    \"Cement Production\",\n",
        "    \"Cement Companies\",\n",
        "    \"Cement Industry\",\n",
        "    \"Cement Stocks\",\n",
        "    \"Cement Demand\",\n",
        "    \"Cement Prices\",\n",
        "    \"Cement Sales\",\n",
        "    \"Cement Plants\",\n",
        "    \"Cement Market\",\n",
        "    \"Cement Production Capacity\",\n",
        "    \"Cement Export\",\n",
        "    \"Cement Import\",\n",
        "    \"Cement Trade\",\n",
        "    \"Cement Infrastructure\",\n",
        "    \"Cement Projects\",\n",
        "    \"Cement Supply Chain\",\n",
        "    \"Cement Consumption\",\n",
        "    \"Cement Manufacturing Companies\",\n",
        "    \"Cement Sector Trends\",\n",
        "    \"Cement Investments\",\n",
        "    \"Cement Regulations\",\n",
        "    \"Cement Production Growth\",\n",
        "    \"Cement Stock Performance\",\n",
        "    \"Cement Sector Analysis\",\n",
        "    \"Cement Sector News\",\n",
        "    \"DCL\",\n",
        "    \"Dewan Cement Limited\",\n",
        "    \"DGKC\",\n",
        "    \"D.G. Khan Cement Company Limited\",\n",
        "    \"DNCCDEF\",\n",
        "    \"Dandot Cement Company Limited\",\n",
        "    \"FCCL\",\n",
        "    \"Fauji Cement Company Limited\",\n",
        "    \"FECTC\",\n",
        "    \"Fecto Cement Limited\",\n",
        "    \"FLYNG\",\n",
        "    \"Flying Cement Company Limited\",\n",
        "    \"GWLC\",\n",
        "    \"Gharibwal Cement Limited\",\n",
        "    \"KOHC\",\n",
        "    \"Kohat Cement Company Limited\",\n",
        "    \"LUCK\",\n",
        "    \"Lucky Cement Limited\",\n",
        "    \"MLCF\",\n",
        "    \"Maple Leaf Cement Factory Limited\",\n",
        "    \"PIOC\",\n",
        "    \"Pioneer Cement Limited\",\n",
        "    \"POWER\",\n",
        "    \"Power Cement Limited\",\n",
        "    \"SMCPL\",\n",
        "    \"Safe Mix Concrete Limited\",\n",
        "    \"THCCL\",\n",
        "    \"Thatta Cement Company Limited\",\n",
        "    \"ZELPDEF\",\n",
        "    \"Zeal Pak Cement Factory Limited\"\n",
        "]\n",
        "economic_keywords = [\n",
        "    \"Trading\", \"Interest Rate\", \"Trade Balance\", \"Federal Board of Revenue\", \"Foreign Direct Investment\",\n",
        "    \"Poverty\", \"Supply and Demand\", \"Economic Cycle\", \"Interest Rates\", \"Market Forces\",\n",
        "    \"Pakistan International Airlines (PIA)\", \"Depression\", \"Productivity\", \"Yield\", \"Recession\",\n",
        "    \"Consumer Price Index (CPI)\", \"Valuable Publications\", \"Central Bank\", \"Market Capitalization\",\n",
        "    \"Trade\", \"Economic Policy\", \"International Monetary Fund (IMF)\", \"Electricity Bills\", \"Decrease in Demand\",\n",
        "    \"Exchange Rate\", \"Innovation\", \"Public Debt\", \"Interest Rate\", \"Dollar\", \"European Central Bank\", \"Money\",\n",
        "    \"Monetary Policy\", \"Inflation\", \"Market Performance\", \"Infrastructure\", \"Innovation\", \"Trade\",\n",
        "    \"Pakistan Stock Exchange (PSX)\", \"Business\", \"Productivity\", \"Prices\", \"Reforms\", \"China-Pakistan Economic Corridor (CPEC)\",\n",
        "    \"Economic Forecast\", \"Taxes\", \"Unemployment\", \"International Monetary Fund (IMF)\", \"Economic Development\", \"Interest Rate\",\n",
        "    \"Economic Policy\", \"Pakistan Peoples Party (PPP)\", \"Competition\", \"State Bank\", \"Infrastructure\", \"Entrepreneurship\",\n",
        "    \"SBP (State Bank of Pakistan)\", \"International Monetary Fund (IMF)\", \"Economic Indicators\", \"Debt\", \"Supply and Demand\",\n",
        "    \"Bill\", \"Commercial Banks\", \"Fiscal Policy\", \"Investment\", \"Inflation\", \"Gross Domestic Product (GDP)\", \"Globalization\",\n",
        "    \"Loan\", \"Billion\", \"Budget\", \"Income Distribution\", \"Capitalization\", \"Labor Force\", \"Surplus\",\n",
        "    \"Indirect Foreign Investment\", \"Shares\", \"Federal Board of Revenue (FBR)\", \"Relief\", \"Producer Price Index (PPI)\",\n",
        "    \"Balance of Relationships\", \"Market Shares\", \"Trade Balance\", \"Exchange\", \"Taxation\", \"Gross Domestic Product (GDP)\",\n",
        "    \"Revenue\", \"Exchange Publication\", \"Economic Cycle\", \"Electricity\", \"Electricity Bills\", \"Inflation\", \"Market Forces\",\n",
        "    \"State Bank of Pakistan\", \"FD\",\n",
        "    \"Budget Deficit\", \"Trade Balance\", \"Consumer Spending\", \"Business Investment\",\n",
        "    \"Consumer Confidence\", \"Economic Growth\", \"Recession\",\n",
        "    \"Economic Indicators\", \"Central Bank\", \"Taxation\", \"Tariffs\",\n",
        "    \"Interest Rate\", \"Trade Balance\", \"Federal Board of Revenue\", \"Foreign Direct Investment\", \"FBR\",\n",
        "    \"Federal Board of Revenue\", \"Poverty\", \"Supply and Demand\", \"Economic Cycle\", \"Interest Rates\", \"Market Forces\",\n",
        "    \"Pakistan International Airlines (PIA)\", \"Productivity\", \"Depression\", \"Yield\",\n",
        "    \"National Debt\", \"Economic Stimulus\", \"Economic Recovery\",\n",
        "    \"Labor Market\", \"Industrial Production\", \"Manufacturing Sector\"\n",
        "]"
      ],
      "metadata": {
        "id": "_raBkW-Jd1cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find sub-categories based on keywords\n",
        "def find_subcategories(row):\n",
        "    sub_categories = []\n",
        "\n",
        "    if not pd.isna(row):  # Check for NaN values\n",
        "        # Tokenize the text\n",
        "        tokens = row.split()\n",
        "\n",
        "        if any(keyword in tokens for keyword in oil_gas_keywords):\n",
        "            sub_categories.append('Oil & Gas')\n",
        "\n",
        "        if any(keyword in tokens for keyword in cement_keywords):\n",
        "            sub_categories.append('Cement')\n",
        "\n",
        "        if any(keyword in tokens for keyword in economic_keywords):\n",
        "            sub_categories.append('Economic')\n",
        "\n",
        "    if not sub_categories:  # No matching sub-categories found\n",
        "        sub_categories.append('NotAssigned')\n",
        "\n",
        "    return sub_categories\n",
        "\n",
        "# Apply the function to each row and create a 'sub-cat' column in df1\n",
        "merged_df['sub-cat'] = merged_df['Cleaned_description'].apply(find_subcategories)\n"
      ],
      "metadata": {
        "id": "nb0lXZeDATrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "economic_rows = merged_df[merged_df['sub-cat'].apply(lambda x: 'Economic' in x)]\n",
        "print(economic_rows)\n",
        "print(economic_rows.shape)"
      ],
      "metadata": {
        "id": "ZxEA_x1vAZBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "economic_news = merged_df.loc[merged_df['sub-cat'].apply(lambda x: 'Economic' in x)]['Headline']\n",
        "print(economic_news)"
      ],
      "metadata": {
        "id": "hPfCUMM_AeTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oil_rows = merged_df[merged_df['sub-cat'].apply(lambda x: 'Oil & Gas' in x)]\n",
        "print(oil_rows)\n",
        "print(oil_rows.shape)"
      ],
      "metadata": {
        "id": "CbGkQaUjAipR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oil_rows = merged_df.loc[merged_df['sub-cat'].apply(lambda x: 'Oil & Gas' in x)]['Headline']\n",
        "print(oil_rows)"
      ],
      "metadata": {
        "id": "W2G15UJyAsng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cement_rows = merged_df[merged_df['sub-cat'].apply(lambda x: 'Cement' in x)]\n",
        "print(cement_rows)\n",
        "print(cement_rows.shape)"
      ],
      "metadata": {
        "id": "fmCzVD_oAxgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cement_rows = merged_df.loc[merged_df['sub-cat'].apply(lambda x: 'Cement' in x)]['Headline']\n",
        "print(cement_rows)"
      ],
      "metadata": {
        "id": "s_5-SHxdA5Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.to_csv(r\"D:\\Sana\\Extra Material\\Updated_2022_Data.csv\", encoding='utf-8',index=False)"
      ],
      "metadata": {
        "id": "sV801fcjBDfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AngwogoLUzRq"
      },
      "source": [
        "# Sentiment Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7RKSqYQlVRp"
      },
      "source": [
        "##### Finetuning Finbert For Sentiment Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T14VXzrSU15p"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/content/all-data (1).csv\", error_bad_lines=False,encoding=\"unicode_escape\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPKnoAxYlYEB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ddARkTTlat3"
      },
      "outputs": [],
      "source": [
        "# Tokenize and encode the news articles\n",
        "news_articles = data['News'].tolist()\n",
        "\n",
        "encoded_data = tokenizer.batch_encode_plus(\n",
        "    news_articles,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "input_ids = encoded_data['input_ids']\n",
        "attention_masks = encoded_data['attention_mask']\n",
        "\n",
        "# Map sentiment labels to numerical values\n",
        "sentiments = data['Sent'].tolist()\n",
        "sentiment_mapping = {'positive': 2, 'negative': 0, 'neutral': 1}\n",
        "labels = [sentiment_mapping[sentiment] for sentiment in sentiments]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzFFURxhljME"
      },
      "outputs": [],
      "source": [
        "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
        "    input_ids, labels, random_state=42, test_size=0.2\n",
        ")\n",
        "\n",
        "train_masks, val_masks, _, _ = train_test_split(\n",
        "    attention_masks, input_ids, random_state=42, test_size=0.2\n",
        ")\n",
        "\n",
        "# Convert train_inputs, train_masks, and train_labels to tensors\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "val_masks = torch.tensor(val_masks)\n",
        "val_labels = torch.tensor(val_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtCVzHMvlotH"
      },
      "outputs": [],
      "source": [
        "# Create Tensor datasets\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "\n",
        "# Set batch size and create data loaders\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Set optimization parameters\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Fine-tune the model\n",
        "epochs = 4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_predictions = []\n",
        "    train_true_labels = []\n",
        "\n",
        "    for batch in train_loader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        train_predictions.extend(logits.argmax(dim=1).tolist())\n",
        "        train_true_labels.extend(batch[2].tolist())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Perform validation after each epoch\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_true_labels = []\n",
        "\n",
        "    for batch in val_loader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "        val_predictions.extend(logits.argmax(dim=1).tolist())\n",
        "        val_true_labels.extend(batch[2].tolist())\n",
        "\n",
        "        val_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    train_accuracy = accuracy_score(train_true_labels, train_predictions)\n",
        "    train_precision = precision_score(train_true_labels, train_predictions, average='weighted')\n",
        "    train_recall = recall_score(train_true_labels, train_predictions, average='weighted')\n",
        "    train_f1 = f1_score(train_true_labels, train_predictions, average='weighted')\n",
        "\n",
        "    val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
        "    val_precision = precision_score(val_true_labels, val_predictions, average='weighted')\n",
        "    val_recall = recall_score(val_true_labels, val_predictions, average='weighted')\n",
        "    val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{epochs}')\n",
        "    print(f'Training loss: {avg_train_loss:.4f}')\n",
        "    print(f'Training accuracy: {train_accuracy:.4f}')\n",
        "    print(f'Training precision: {train_precision:.4f}')\n",
        "    print(f'Training recall: {train_recall:.4f}')\n",
        "    print(f'Training F1-score: {train_f1:.4f}')\n",
        "    print(f'Validation loss: {avg_val_loss:.4f}')\n",
        "    print(f'Validation accuracy: {val_accuracy:.4f}')\n",
        "    print(f'Validation precision: {val_precision:.4f}')\n",
        "    print(f'Validation recall: {val_recall:.4f}')\n",
        "    print(f'Validation F1-score: {val_f1:.4f}\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPMUi-r1lqZ-"
      },
      "outputs": [],
      "source": [
        "# Define the directory path to save and load the model\n",
        "save_directory = '/content/drive/MyDrive/Datasets/finetuned_model/'\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B-9W_nwZ_-D"
      },
      "source": [
        "##### Loading Saved Finbert For Sentiment Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIhoQDaslr1e"
      },
      "source": [
        "(breaking news into sentences and then getting sentiment of each sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0zc_YJzlxNP"
      },
      "outputs": [],
      "source": [
        "save_directory = '/content/drive/MyDrive/finetuned_model/'\n",
        "\n",
        "# Load the fine-tuned model\n",
        "loaded_model = AutoModelForSequenceClassification.from_pretrained(save_directory)\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7k_19TiIly7Y"
      },
      "outputs": [],
      "source": [
        "news_articles =  merged_df['Combined_Cleaned'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YreuxDxAlzad"
      },
      "outputs": [],
      "source": [
        "# Tokenize and encode the news articles\n",
        "encoded_data = loaded_tokenizer.batch_encode_plus(\n",
        "    news_articles,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "input_ids = encoded_data['input_ids']\n",
        "attention_masks = encoded_data['attention_mask']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMGODpadl0xD"
      },
      "outputs": [],
      "source": [
        "# Initialize the sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qlgo4zECl16q"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "loaded_model = loaded_model.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "loaded_model.eval()\n",
        "\n",
        "# Define the mapping from numerical labels to sentiment categories\n",
        "numerical_to_sentiment = {2: 'positive', 0: 'negative', 1: 'neutral'}\n",
        "\n",
        "# Function to calculate the majority sentiment\n",
        "def calculate_majority_sentiment(sentiments):\n",
        "    positive_count = sentiments.count('positive')\n",
        "    negative_count = sentiments.count('negative')\n",
        "    neutral_count = sentiments.count('neutral')\n",
        "\n",
        "    if positive_count > negative_count and positive_count > neutral_count:\n",
        "        return 'positive'\n",
        "    elif negative_count > positive_count and negative_count > neutral_count:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "# Apply sentiment analysis to each sentence\n",
        "sentiment_predictions = []\n",
        "for description in merged['Combined_Cleaned']:\n",
        "    sentences = nltk.sent_tokenize(description)\n",
        "    sentence_sentiments = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        encoded_input = loaded_tokenizer(sentence, truncation=True, padding=True, return_tensors='pt')\n",
        "        input_ids = encoded_input['input_ids'].to(device)\n",
        "        attention_mask = encoded_input['attention_mask'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = loaded_model(input_ids, attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        predicted_class = torch.argmax(logits, dim=1).item()\n",
        "        sentiment = numerical_to_sentiment[predicted_class]\n",
        "        sentence_sentiments.append(sentiment)\n",
        "\n",
        "    majority_sentiment = calculate_majority_sentiment(sentence_sentiments)\n",
        "    sentiment_predictions.append(majority_sentiment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdfA37ZBl3P8"
      },
      "outputs": [],
      "source": [
        "# Add the sentiment predictions as a new column\n",
        "merged_df['sentiment'] = sentiment_predictions\n",
        "\n",
        "# Drop the unnamed columns\n",
        "unnamed_columns = [col for col in merged_df.columns if 'Unnamed' in col]\n",
        "merged_df = merged_df.drop(columns=unnamed_columns)\n",
        "\n",
        "# Save the updated dataframe to a CSV file\n",
        "merged_df.to_csv('updated.csv', index=False)\n",
        "\n",
        "# Download the updated CSV file\n",
        "from google.colab import files\n",
        "files.download('updated.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HexRyBBl42c"
      },
      "outputs": [],
      "source": [
        "# Create a mapping dictionary\n",
        "sentiment_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "\n",
        "# Apply the mapping to the 'sentiment' column\n",
        "merged_df['sentiment'] = merged_df['sentiment'].map(sentiment_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCX6ohcjmadl"
      },
      "source": [
        "# Keyword Extraction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract keywords using TextRank\n",
        "def extract_keywords_textrank(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha() and len(word) > 2]\n",
        "        filtered_sentence = ' '.join(filtered_words)\n",
        "        filtered_sentences.append(filtered_sentence)\n",
        "\n",
        "    G = nx.Graph()\n",
        "    sentence_tokens = [word_tokenize(sentence) for sentence in filtered_sentences]\n",
        "\n",
        "    for sentence in sentence_tokens:\n",
        "        G.add_nodes_from(sentence)\n",
        "\n",
        "    window_size = 2\n",
        "    for sentence in sentence_tokens:\n",
        "        for i, word in enumerate(sentence):\n",
        "            for j in range(i + 1, min(i + window_size, len(sentence))):\n",
        "                G.add_edge(sentence[i], sentence[j])\n",
        "\n",
        "    scores = nx.pagerank(G)\n",
        "    num_keywords = 20\n",
        "    keywords = sorted(scores, key=scores.get, reverse=True)[:num_keywords]\n",
        "\n",
        "    return keywords\n",
        "\n",
        "def extract_keywords_tfidf(text):\n",
        "    # Create a TfidfVectorizer\n",
        "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "    # Fit and transform your text to TF-IDF features\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
        "\n",
        "    # Get the feature names (words)\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Get the TF-IDF values for the words in the text\n",
        "    tfidf_values = tfidf_matrix.toarray()\n",
        "\n",
        "    # Calculate the average TF-IDF value for each word\n",
        "    average_tfidf_values = tfidf_values.mean(axis=0)\n",
        "\n",
        "    # Sort the words by their average TF-IDF values\n",
        "    keywords = [feature_names[i] for i in average_tfidf_values.argsort()[::-1] if len(feature_names[i]) > 2]\n",
        "\n",
        "    # Select the top N keywords\n",
        "    num_keywords = 20\n",
        "    top_keywords = keywords[:num_keywords]\n",
        "\n",
        "    return top_keywords\n",
        "\n",
        "# Function to extract keywords using KeyBERT\n",
        "def extract_keywords_keybert(text):\n",
        "    model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
        "    keywords = model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words='english', top_n=20)\n",
        "    return [kw[0] for kw in keywords if len(kw[0]) > 2]\n",
        "\n",
        "# Function to extract keywords using spaCy\n",
        "def extract_keywords_spacy(text):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(text)\n",
        "    keywords = [token.text for token in doc if not token.is_stop and len(token.text) > 2 and token.pos_ not in ['VERB', 'ADJ']][:20]\n",
        "    return keywords\n",
        "\n",
        "# Apply the functions to each row in the DataFrame\n",
        "merged_df['Keywords_TextRank'] = merged_df['CC_wo_Stem_Lema_Punc'].apply(extract_keywords_textrank)\n",
        "merged_df['Keywords_TFIDF'] = merged_df['CC_wo_Stem_Lema_Punc'].apply(extract_keywords_tfidf)\n",
        "merged_df['Keywords_KeyBERT'] = merged_df['CC_wo_Stem_Lema_Punc'].apply(extract_keywords_keybert)\n",
        "merged_df['Keywords_spaCy'] = merged_df['CC_wo_Stem_Lema_Punc'].apply(extract_keywords_spacy)"
      ],
      "metadata": {
        "id": "Gt6HJH5bVJbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert lists to strings before saving to CSV\n",
        "merged_df['Keywords_TextRank'] = merged_df['Keywords_TextRank'].apply(lambda x: ', '.join(x) if isinstance(x, list) else '')\n",
        "merged_df['Keywords_TFIDF'] = merged_df['Keywords_TFIDF'].apply(lambda x: ', '.join(x) if isinstance(x, list) else '')\n",
        "merged_df['Keywords_KeyBERT'] = merged_df['Keywords_KeyBERT'].apply(lambda x: ', '.join(x) if isinstance(x, list) else '')\n",
        "merged_df['Keywords_spaCy'] = merged_df['Keywords_spaCy'].apply(lambda x: ', '.join(x) if isinstance(x, list) else '')"
      ],
      "metadata": {
        "id": "MLMogZcJVR6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all words to lowercase in the Keywords_CC column and combine them into a single list\n",
        "merged_df['Keywords_CC'] = merged_df.apply(lambda row: [word.lower() for col in ['Keywords_TextRank', 'Keywords_TFIDF', 'Keywords_KeyBERT', 'Keywords_spaCy'] for word in row[col].split(', ')], axis=1)\n",
        "\n",
        "# Convert the combined keywords to a set to get unique words\n",
        "merged_df['Keywords_CC'] = merged_df['Keywords_CC'].apply(set)\n",
        "\n",
        "# Convert the set back to a list\n",
        "merged_df['Keywords_CC'] = merged_df['Keywords_CC'].apply(list)\n",
        "\n",
        "# Drop the intermediate columns\n",
        "merged_df = merged_df.drop(columns=['Keywords_TextRank', 'Keywords_TFIDF', 'Keywords_KeyBERT', 'Keywords_spaCy'])\n"
      ],
      "metadata": {
        "id": "K7O56uLYVUDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.head(1)"
      ],
      "metadata": {
        "id": "aXFyh2nQVbV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueBqM0GAmtTA"
      },
      "source": [
        "# Entity Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX0K4AZCmuU5"
      },
      "source": [
        "##### Using Flair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPWqfRrWmwdf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from flair.models import SequenceTagger\n",
        "from flair.data import Sentence\n",
        "\n",
        "# Load the Flair named entity recognition (NER) tagger\n",
        "tagger = SequenceTagger.load('\"flair/ner-english\"')\n",
        "\n",
        "# Define a function to extract entities from a text using Flair\n",
        "def extract_entities(text):\n",
        "    sentence = Sentence(text)\n",
        "    tagger.predict(sentence)\n",
        "\n",
        "    persons = []\n",
        "    organizations = []\n",
        "    others = []\n",
        "\n",
        "    for entity in sentence.get_spans('ner'):\n",
        "        if entity.tag == 'PER':\n",
        "            persons.append(entity.text)\n",
        "        elif entity.tag == 'ORG':\n",
        "            organizations.append(entity.text)\n",
        "        else:\n",
        "            others.append(entity.text)\n",
        "\n",
        "    return persons, organizations, others\n",
        "\n",
        "#merged_first_10 = merged.head(10).copy()\n",
        "#merged_first_10 = merged.copy()\n",
        "\n",
        "# Initialize empty lists to store the entities for each row\n",
        "persons_list = []\n",
        "organizations_list = []\n",
        "others_list = []\n",
        "\n",
        "# Apply the function to each row of the first 10 instances and store entities in the lists\n",
        "for _, row in merged_df.iterrows():\n",
        "    entities_data = extract_entities(row['CC_wo_Stopwords'])\n",
        "    persons_list.append(entities_data[0])\n",
        "    organizations_list.append(entities_data[1])\n",
        "    others_list.append(entities_data[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8-dFMW0mx-O"
      },
      "outputs": [],
      "source": [
        "# Convert the lists to sets to remove duplicates, then back to lists\n",
        "persons_list = [list(set(p)) for p in persons_list]\n",
        "organizations_list = [list(set(o)) for o in organizations_list]\n",
        "others_list = [list(set(o)) for o in others_list]\n",
        "\n",
        "# Assign the lists to the DataFrame columns\n",
        "merged_df['NER_Persons'] = persons_list\n",
        "merged_df['NER_Organizations'] = organizations_list\n",
        "merged_df['NER_Others'] = others_list\n",
        "\n",
        "# Now, the \"NER_Persons\", \"NER_Organizations\", and \"NER_Others\" columns contain unique lists of entities for the first 10 instances\n",
        "print(merged_df[[\"NER_Persons\", \"NER_Organizations\", \"NER_Others\"]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8OYieP1qt-p"
      },
      "outputs": [],
      "source": [
        "merged_df.to_csv('updated.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSJdSSUETKDN"
      },
      "source": [
        "# Load Roberta For Embedding Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCwTPxYkV6NF"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Define your RoBERTa model name\n",
        "roberta_model_name = \"roberta-base\"\n",
        "\n",
        "# Load the RoBERTa model and tokenizer\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name)\n",
        "roberta_model = AutoModel.from_pretrained(roberta_model_name)\n",
        "\n",
        "# Define the maximum sequence length\n",
        "max_seq_length = 512  # You can adjust this based on your model's capabilities\n",
        "\n",
        "# Create a tqdm instance to track progress\n",
        "tqdm.pandas()\n",
        "\n",
        "# Create an empty list to store the RoBERTa embeddings\n",
        "roberta_embeddings = []\n",
        "\n",
        "# Process each row in your dataset\n",
        "for text in tqdm(merged_df['CC_wo_Stem_Lema_Punc']):\n",
        "    # Truncate the text if it exceeds the maximum sequence length\n",
        "    text = text[:max_seq_length]\n",
        "\n",
        "    # RoBERTa embeddings\n",
        "    inputs = roberta_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        output = roberta_model(**inputs)\n",
        "    roberta_embedding = output.last_hidden_state.mean(dim=1).numpy()\n",
        "    roberta_embeddings.append(roberta_embedding)\n",
        "\n",
        "# Add a new 'Embedding_Vector' column to the DataFrame\n",
        "merged_df['Embedding_Vector'] = roberta_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Event Extraction Functions"
      ],
      "metadata": {
        "id": "-Otc8jimB-Dg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Common Functions"
      ],
      "metadata": {
        "id": "2TxusAXHpTCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine(x, y):\n",
        "    x = x.flatten()\n",
        "    y = y.flatten()\n",
        "    return 1 - distance.cosine(x, y)\n",
        "\n",
        "\n",
        "def intersection(lst1, lst2):\n",
        "    # Use of hybrid method\n",
        "    temp = set(lst2)\n",
        "    lst3 = [value for value in lst1 if value in temp]\n",
        "    return lst3\n",
        "\n",
        "def intersection_rate(words_list_, words_list1_):\n",
        "    if not words_list_ or not words_list1_:\n",
        "        return 0.0\n",
        "\n",
        "    # Citation calc\n",
        "    min_ = words_list_.copy()\n",
        "    max_ = words_list1_.copy()\n",
        "    if len(max_) < len(min_):\n",
        "        min_ = words_list1_.copy()\n",
        "        max_ = words_list_.copy()\n",
        "\n",
        "    intersection_list = intersection(min_, max_)\n",
        "    intersection_rate_ = len(intersection_list) / len(min_)\n",
        "\n",
        "    return intersection_rate_\n",
        "\n",
        "\n",
        "def similarity_score(sim_cos, citation):\n",
        "    return sim_cos * 0.8 + citation * 0.2"
      ],
      "metadata": {
        "id": "cAwdIYMxpTwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5plZI8M6TAiU"
      },
      "source": [
        "##### Majority Voting and Purity Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNkdAl8ZVyqC"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def majority_voting(elements):\n",
        "    if not elements:\n",
        "        return None\n",
        "\n",
        "    counter = Counter(elements)\n",
        "    majority_count = max(counter.values())\n",
        "    majority_elements = [element for element, count in counter.items() if count == majority_count]\n",
        "\n",
        "    if len(majority_elements) == 1:\n",
        "        return majority_elements[0]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "elements = ['pak', 'biz', 'pak', 'ent', 'biz', 'world', 'biz', 'pak']\n",
        "majority = majority_voting(elements)\n",
        "\n",
        "if majority is not None:\n",
        "    print(f\"The majority element is: {majority}\")\n",
        "else:\n",
        "    print(\"No majority element found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9QDpu56V041"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def calculate_majority_purity(elements):\n",
        "    if not elements:\n",
        "        return None\n",
        "\n",
        "    counter = Counter(elements)\n",
        "    # print(f\"counter: {counter}\")\n",
        "    majority_count = max(counter.values())\n",
        "    # print(f\"majoity_count: {majority_count}\")\n",
        "    majority_elements = [element for element, count in counter.items() if count == majority_count]\n",
        "    # print(f\"majority_elements: {majority_elements}\")\n",
        "\n",
        "    majority_percentage = (majority_count / len(elements))\n",
        "    return majority_percentage\n",
        "\n",
        "elements = ['pak', 'pak', 'pak', 'pak', 'biz', 'world', 'pak', 'pak']\n",
        "purity = calculate_majority_purity(elements)\n",
        "\n",
        "if purity is not None:\n",
        "    print(f\"The purity of the majority element is: {purity}\")\n",
        "else:\n",
        "    print(\"No majority element found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSbCamw8V2Bh"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "if pos = neg --> neu\n",
        "if pos = neg = neu --> neu\n",
        "if pos = neu --> pos\n",
        "if neg = neu --> neg\n",
        "'''\n",
        "from collections import Counter\n",
        "\n",
        "def find_majority(votes):\n",
        "    vote_count = Counter(votes)\n",
        "    top_two = vote_count.most_common(2)\n",
        "    if len(top_two)>1 and top_two[0][1] == top_two[1][1]:\n",
        "        if top_two[0][0] != 1 and top_two[1][0] != 1:\n",
        "          # It is a tie so neutral\n",
        "          return 1     # 0 --> neg, 1 --> neu, 2 --> pos\n",
        "        elif top_two[0][0] == 1:\n",
        "          return top_two[1][0]\n",
        "        elif top_two[1][0] == 1:\n",
        "          return top_two[0][0]\n",
        "    return top_two[0][0]\n",
        "\n",
        "find_majority([0,0,0,2,2,1,1,1]) # It is a tie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbNSITfDTFTJ"
      },
      "source": [
        "##### Category Priority Wise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcRmijeUTF5g"
      },
      "outputs": [],
      "source": [
        "merged['main_cat'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcGg33UIQolh"
      },
      "outputs": [],
      "source": [
        "def select_priority_category(categories):\n",
        "    priority_mapping = {\n",
        "        'Business': 1,\n",
        "        'International': 2,\n",
        "        'National': 3,\n",
        "        'Print': 4,\n",
        "        'Editorial': 5,\n",
        "        'Opinion': 6\n",
        "    }\n",
        "\n",
        "    selected_category = None\n",
        "    selected_priority = float('inf')\n",
        "\n",
        "    for category in categories:\n",
        "        priority = priority_mapping.get(category, float('inf'))\n",
        "        if priority < selected_priority:\n",
        "            selected_category = category\n",
        "            selected_priority = priority\n",
        "\n",
        "    return selected_category"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-pawAzPqxv6"
      },
      "source": [
        "# Event Extraction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace ['NotAssigned'] with an empty list\n",
        "merged_df['sub-cat'] = merged_df['sub-cat'].apply(lambda x: [] if x == \"['NotAssigned']\" else x)"
      ],
      "metadata": {
        "id": "Es881up4DpYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of ['NotAssigned'] in the 'sub-cat' column\n",
        "count_not_assigned = (merged_df['sub-cat'] == \"['Oil & Gas']\").sum()\n",
        "\n",
        "print(f\"Number of rows with ['NotAssigned'] in the 'sub-cat' column: {count_not_assigned}\")"
      ],
      "metadata": {
        "id": "-WAt8HUxDqJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'DateTime' column to datetime\n",
        "merged_df['DateTime'] = pd.to_datetime(merged_df['DateTime'])\n",
        "\n",
        "# Define the start and end dates for the first 6 months of 2022\n",
        "#start_date = pd.Timestamp('2022-01-01')\n",
        "#end_date = pd.Timestamp('2022-12-31')\n",
        "\n",
        "# Filter rows with news from the first 6 months of 2022\n",
        "#test_df = merged_df[(merged_df['DateTime'] >= start_date) & (merged_df['DateTime'] <= end_date)]\n",
        "\n",
        "# To make a copy of the DataFrame, use .copy()\n",
        "#test_df = test_df.copy()\n",
        "\n",
        "# Display the 'test_df'\n",
        "#print(test_df.shape)"
      ],
      "metadata": {
        "id": "CMoOLlRoDvg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_df.index.min())  # Minimum valid index\n",
        "print(merged_df.index.max())  # Maximum valid index\n"
      ],
      "metadata": {
        "id": "etiJsfqYDS_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to convert string representation of lists back to actual lists\n",
        "def convert_str_to_list(string):\n",
        "    try:\n",
        "        # Use ast.literal_eval to safely evaluate the string as a list\n",
        "        return ast.literal_eval(string)\n",
        "    except (ValueError, SyntaxError):\n",
        "        # If the string cannot be converted to a list, return it as is\n",
        "        return string"
      ],
      "metadata": {
        "id": "8hJccBsVD40h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def string_to_array(string_value):\n",
        "    # Remove the square brackets from the string\n",
        "    string_value = string_value.strip('[]')\n",
        "\n",
        "    # Convert the string to a numpy array\n",
        "    array_value = np.fromstring(string_value, sep=' ')\n",
        "\n",
        "    # Reshape the array to its original shape\n",
        "    return array_value.reshape((768,))\n",
        "\n",
        "merged_df['Embedding_Vector'] = merged_df['Embedding_Vector'].apply(string_to_array)"
      ],
      "metadata": {
        "id": "KkzbVIlbD5-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a random seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Get the total number of rows in your DataFrame\n",
        "num_rows = merged_df.shape[0]\n",
        "\n",
        "# Define the number of random samples you want (e.g., 10)\n",
        "num_samples = 10\n",
        "\n",
        "# Get a random sample of row indices\n",
        "random_indices = random.sample(range(num_rows), num_samples)\n",
        "\n",
        "# Loop through the random indices and print the shape of the embedding vectors\n",
        "for index in random_indices:\n",
        "    row = merged_df.iloc[index]\n",
        "    embedding_vector = row['Embedding_Vector']\n",
        "    if isinstance(embedding_vector, np.ndarray):\n",
        "        embedding_shape = embedding_vector.shape\n",
        "    else:\n",
        "        embedding_shape = None  # Handle cases where parsing failed\n",
        "    print(f\"Shape of embedding vector at index {index}: {embedding_shape}\")"
      ],
      "metadata": {
        "id": "V67XhAjiD8Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "cols = ['Keywords_CC',\t'NER_Persons', 'NER_Organizations', 'NER_Others','sub-cat']\n",
        "for col in cols:\n",
        "  merged_df[col] = merged_df[col].apply(convert_str_to_list)"
      ],
      "metadata": {
        "id": "nJPufFkcD_K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in merged_df.columns:\n",
        "    print(column, type(merged_df[column][2891]))"
      ],
      "metadata": {
        "id": "obF0o7_mEE8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e1 = merged_df['Embedding_Vector'][1361]\n",
        "e2 = merged_df['Embedding_Vector'][1362]\n",
        "print(type(e1))\n",
        "print(type(e2))\n",
        "print(e1)"
      ],
      "metadata": {
        "id": "h7FNEZQ8DTil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e1 = merged_df['Embedding_Vector'][1361]\n",
        "e2 = merged_df['Embedding_Vector'][1500]\n",
        "print(merged_df['Headline'].iloc[1361])\n",
        "print(merged_df['Headline'].iloc[1500])\n",
        "sim_cos = cosine(e1, e2)\n",
        "print(sim_cos)"
      ],
      "metadata": {
        "id": "HsgV9_o7DWQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a mapping from sentiment labels to numeric values\n",
        "sentiment_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "\n",
        "# Apply the mapping using a lambda function\n",
        "merged_df['Sentiment_Prediction'] = merged_df['Sentiment_Prediction'].apply(lambda x: sentiment_mapping.get(x, x))\n"
      ],
      "metadata": {
        "id": "6kHzXoeeDabw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def event_extraction(df):\n",
        "  # Add a print statement to see if the function is called\n",
        "  print(\"Event extraction function called\")\n",
        "\n",
        "  added_urls = set()\n",
        "  sim_score = []  # Initialize the sim_score list\n",
        "  for ind, row in df.iterrows():\n",
        "    headlines, sources, urls, tareekhs, desc = [], [], [], [], []\n",
        "    cats, sim_score, sentiments, sub_cat = [], [], [], []\n",
        "\n",
        "    ner, keywords = row['NER_Persons'], row['Keywords_CC']\n",
        "    url = row['Url']\n",
        "    sim_score.append(-1)\n",
        "    sentiments.append(row['Sentiment_Prediction'])\n",
        "    cats.append(row['main_cat'])\n",
        "    urls.append(url)\n",
        "    sources.append(row['source'])\n",
        "    tareekhs.append(row['DateTime'])\n",
        "    desc.append(row['News'])\n",
        "    sub_cat.append(row['sub-cat'])\n",
        "    headlines.append(row['Headline'])\n",
        "\n",
        "    # Skip url if already part of a previous event\n",
        "    if url not in added_urls:\n",
        "      vector1 = row['Embedding_Vector']\n",
        "      # Get Words list of the main article\n",
        "      article = row['CC_wo_Stopwords_Punc']\n",
        "      article  = re.sub(r'(\\d+)(\\D)', r'\\1 \\2', article)\n",
        "      article  = re.sub(r'(\\d+)', r' \\1', article)\n",
        "      words = article.replace('nbsp', '').split()\n",
        "\n",
        "      # Get a subset of indexes greater than the index of currrent article\n",
        "      all_indexes = df.index.tolist()\n",
        "      subset_indexes = [index for index in all_indexes if index > ind]\n",
        "\n",
        "      # Compare with all the other articles in the dataframe\n",
        "      # similarities = []\n",
        "      for index in subset_indexes:\n",
        "        article2 = df.iloc[index]['CC_wo_Stopwords_Punc']\n",
        "        article2  = re.sub(r'(\\d+)(\\D)', r'\\1 \\2', article2)\n",
        "        article2  = re.sub(r'(\\d+)', r' \\1', article2)\n",
        "        words2 = article2.replace('nbsp', '').split()\n",
        "\n",
        "        vector2 = df.iloc[index]['Embedding_Vector']\n",
        "\n",
        "        # Find Similarity/Citation\n",
        "        citation = intersection_rate(words, words2)\n",
        "        sim_cos = cosine(vector1, vector2)\n",
        "        score = similarity_score(sim_cos, citation)\n",
        "\n",
        "\n",
        "        if score > 0.6 and (citation > 0.5):\n",
        "          new_url = df.iloc[index]['Url']\n",
        "          if new_url not in added_urls:   # Check if the url has already been identified as part of another event\n",
        "            urls.append(new_url)\n",
        "            added_urls.add(new_url)     # Add the new url to the added_urls set to avoid repetition\n",
        "            sources.append(df.iloc[index]['source'])\n",
        "            tareekhs.append(df.iloc[index]['DateTime'])\n",
        "            desc.append(df.iloc[index]['News'])\n",
        "            cats.append(df.iloc[index]['main_cat'])\n",
        "            headlines.append(df.iloc[index]['Headline'])\n",
        "            sentiments.append(df.iloc[index]['Sentiment_Prediction'])\n",
        "            sub_cat.append(df.iloc[index]['sub-cat'])\n",
        "            keywords += df.iloc[index]['Keywords_CC']\n",
        "            ner += df.iloc[index]['NER_Persons']\n",
        "            if -1 in sim_score:\n",
        "              sim_score.remove(-1)\n",
        "            sim_score.append(score)\n",
        "\n",
        "            #print(f\"Citation: {citation}\\nCosine: {sim_cos}\\nScore: {score}\")\n",
        "            # print(urls)\n",
        "            # print(\"----------------------------------------\")\n",
        "\n",
        "      url_category_dict = {}\n",
        "      url_subcat_dict = {}\n",
        "      url_senti_dict = {}\n",
        "      for url, category in zip(urls, cats):\n",
        "        if url not in url_category_dict:\n",
        "            url_category_dict[url] = category\n",
        "\n",
        "      for url, sub in zip(urls, sub_cat):\n",
        "         if url not in url_subcat_dict:\n",
        "             url_subcat_dict[url] = sub\n",
        "\n",
        "      for url, sentiment in zip(urls, sentiments):\n",
        "        if url not in url_senti_dict:\n",
        "            url_senti_dict[url] = sentiment\n",
        "\n",
        "      # Separate the unique URLs and categories from the dictionary\n",
        "      urls = list(url_category_dict.keys())\n",
        "      cats = list(url_category_dict.values())\n",
        "      sub_cat = list(url_subcat_dict.values())\n",
        "      # Flatten the list of lists and then convert it to a set to remove duplicates\n",
        "      sub_cat = list(set(sub_category for sublist in sub_cat for sub_category in sublist))\n",
        "      sentiments = list(url_senti_dict.values())\n",
        "      sources = list(set(sources))\n",
        "      desc = list(set(desc))\n",
        "      keywords = list(set(keywords))\n",
        "      ner = list(set(ner))\n",
        "      maj_cat = majority_voting(cats)\n",
        "      priority = select_priority_category(cats)\n",
        "      purity = calculate_purity(priority, cats)\n",
        "      maj_purity = calculate_majority_purity(cats)\n",
        "      maj_senti = find_majority(sentiments)\n",
        "      new_row = {'Event': df.iloc[ind]['Headline'], 'Sources': sources,'Headlines':headlines, 'Urls': urls, 'Description': desc, 'Articles': len(urls),\n",
        "                  'Categories': cats, 'Sub_Category': sub_cat, 'Major_Cat': maj_cat, 'Priority_cat': priority, 'Priority_Purity': purity, 'Maj_Purity': maj_purity,\n",
        "                  'Sentiment_Prediction': maj_senti, 'Keywords': keywords, 'Similarity_Scores': sim_score, 'NER_Persons': ner, 'Start Date': start_date, 'End Date': end_date}\n",
        "      global events\n",
        "      events = pd.concat([events, pd.DataFrame([new_row])], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "IJVvmN82EWef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### One Day Events"
      ],
      "metadata": {
        "id": "-0ifCfUPEn7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "\n",
        "\n",
        "# Convert the \"DateTime\" column to a datetime format\n",
        "test_df['DateTime'] = pd.to_datetime(test_df['DateTime'])\n",
        "\n",
        "# Remove the timestamp (keep only the date part)\n",
        "test_df['DateTime'] = test_df['DateTime'].dt.date\n",
        "\n",
        "# Find the oldest and latest dates\n",
        "oldest_date = test_df['DateTime'].min()\n",
        "latest_date = test_df['DateTime'].max()\n",
        "\n",
        "print(\"Oldest Date:\", oldest_date)\n",
        "print(\"Latest Date:\", latest_date)\n",
        "\n",
        "dates = list(test_df['DateTime'].unique())\n",
        "dates = np.sort(dates)\n",
        "print(len(dates), dates[0:5])\n",
        "\n",
        "# List of column names\n",
        "cols = ['Event', 'Sources', 'Urls', 'Articles', 'Start Date', 'End Date',]\n",
        "# Create an empty dataframe to store all the events\n",
        "global events\n",
        "events = pd.DataFrame(columns=cols)\n",
        "\n",
        "for date in dates:\n",
        "  global start_date\n",
        "  global end_date\n",
        "  start_date = date\n",
        "  end_date = date\n",
        "\n",
        "  df = test_df[test_df['DateTime']==date]\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  print(f\"START_DATE: {start_date}, END_DATE: {end_date}\")\n",
        "  event_extraction(df)\n",
        "\n",
        "events.to_csv(r\"D:\\Sana\\Extra Material\\July_Dec_Events.csv\", encoding='utf-8', index=False)\n",
        "print(f\"Columns: {events.columns}\\nShape: {events.shape}\")\n",
        "events.head(3)"
      ],
      "metadata": {
        "id": "pfAZL-M8Ep8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "events['Urls'] = events['Urls'].apply(tuple)\n",
        "\n",
        "print(f\"Shape Before Duplicate Removal: {events.shape}\")\n",
        "# events_all1.drop_duplicates(inplace=True)\n",
        "events.drop_duplicates(subset='Urls' ,inplace=True)\n",
        "events.reset_index(drop=True)\n",
        "print(\".......................................\")\n",
        "print(f\"Shape After Duplicate Removal: {events.shape}\")\n",
        "\n",
        "\n",
        "indsub1, indsub2 = [], []\n",
        "articles = np.sort(events['Articles'].unique())\n",
        "for art in articles:\n",
        "  sub1 = events[events['Articles']==art]\n",
        "  sub2 = events[events['Articles']>art]\n",
        "  for index, row in sub1.iterrows():\n",
        "    url = row['Urls']\n",
        "    # print(type(url), url)\n",
        "    for ind, row2 in sub2.iterrows():\n",
        "      urls = row2['Urls']\n",
        "      if set(url).issubset(set(urls)):\n",
        "        indsub1.append(index)\n",
        "        indsub2.append(ind)\n",
        "  # print(art, sub1.shape, sub2.shape)\n",
        "\n",
        "print(f\"indeces dropped: {len(set(indsub1))}\")\n",
        "\n",
        "# Drop rows by indices\n",
        "events.drop(indsub1, inplace=True)\n",
        "\n",
        "# Reset index after dropping rows\n",
        "events.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(events.shape)"
      ],
      "metadata": {
        "id": "ELpe9Rb-Exvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "economic_rows = merge_df[merge_df['Sub_Category'].apply(lambda x: isinstance(x, list) and 'Economic' in x if isinstance(x, (list, np.ndarray)) else ('Economic' in x if isinstance(x, str) else False))]\n",
        "print(economic_rows.shape)"
      ],
      "metadata": {
        "id": "BCYkfV4hWv0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Five Day Events"
      ],
      "metadata": {
        "id": "KKEgL5u0EfjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "merged_df['DateTime'] = pd.to_datetime(merged_df['DateTime'])\n",
        "\n",
        "# # Find the oldest and latest dates\n",
        "oldest_date5 = merged_df['DateTime'].min()\n",
        "latest_date5 = merged_df['DateTime'].max()\n",
        "\n",
        "print(\"Oldest Date:\", oldest_date5)\n",
        "print(\"Latest Date:\", latest_date5)\n",
        "\n",
        "dates5 = list(merged_df['DateTime'].unique())\n",
        "dates5 = np.sort(dates5)\n",
        "dates5 = pd.date_range(start=oldest_date5, end=latest_date5, freq='D')\n",
        "print(len(dates5), dates5[0:5])\n",
        "\n",
        "# List of column names\n",
        "cols = ['Event', 'Sources', 'Urls', 'Articles', 'Start Date', 'End Date']\n",
        "\n",
        "# Create an empty dataframe to store all the events\n",
        "global events\n",
        "events = pd.DataFrame(columns=cols)\n",
        "\n",
        "for i in range(4, len(dates5)):\n",
        "    window_dates = dates5[i-4:i+1]  # Get the current 5-day window\n",
        "    # Find the oldest and latest dates\n",
        "    global start_date\n",
        "    global end_date\n",
        "    start_date = min(window_dates)\n",
        "    end_date = max(window_dates)\n",
        "    #print(f\"Window: Dates: {window_dates}\\n start: {start_date}\\nend: {end_date}\")\n",
        "    # break\n",
        "    df = merged_df[merged_df['DateTime'].isin(window_dates)]  # Filter dataframe within the window dates\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    print(f\"START_DATE: {start_date}, END_DATE: {end_date}\")\n",
        "    event_extraction(df)\n",
        "\n",
        "events_all5 = events\n",
        "print(f\"Columns: {events_all5.columns}\\nShape: {events_all5.shape}\")\n",
        "events_all5.head(3)\n"
      ],
      "metadata": {
        "id": "Rd0gH-RNEiW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "events_all5['Urls'] = events_all5['Urls'].apply(tuple)\n",
        "\n",
        "print(f\"Shape Before Duplicate Removal: {events_all5.shape}\")\n",
        "# events_all1.drop_duplicates(inplace=True)\n",
        "events_all5.drop_duplicates(subset='Urls' ,inplace=True)\n",
        "events_all5.reset_index(drop=True)\n",
        "print(\".......................................\")\n",
        "print(f\"Shape After Duplicate Removal: {events_all5.shape}\")\n",
        "\n",
        "\n",
        "indsub1, indsub2 = [], []\n",
        "articles = np.sort(events_all5['Articles'].unique())\n",
        "for art in articles:\n",
        "  sub1 = events_all5[events_all5['Articles']==art]\n",
        "  sub2 = events_all5[events_all5['Articles']>art]\n",
        "  for index, row in sub1.iterrows():\n",
        "    url = row['Urls']\n",
        "    # print(type(url), url)\n",
        "    for ind, row2 in sub2.iterrows():\n",
        "      urls = row2['Urls']\n",
        "      if set(url).issubset(set(urls)):\n",
        "        indsub1.append(index)\n",
        "        indsub2.append(ind)\n",
        "  # print(art, sub1.shape, sub2.shape)\n",
        "\n",
        "print(f\"indeces dropped: {len(set(indsub1))}\")\n",
        "\n",
        "# Drop rows by indices\n",
        "events_all5.drop(indsub1, inplace=True)\n",
        "\n",
        "# Reset index after dropping rows\n",
        "events_all5.reset_index(drop=True, inplace=True)\n",
        "\n",
        "events_all5.to_csv(r\"D:\\Sana\\Extra Material\\5Days_Events.csv\", encoding='utf-8', index=False)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(events_all5.shape)"
      ],
      "metadata": {
        "id": "1-JKb3syEjdV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}